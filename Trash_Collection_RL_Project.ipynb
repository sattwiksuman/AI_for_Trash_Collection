{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trash_Collection_RL_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb82356fb8194b489c2a03008c8124ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9b2a42f6c6ca4478b2e9b650286b8a7f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72ffa151638043c8968f3e2696291eb0",
              "IPY_MODEL_5774ffca7d86492580ac61d025d06770"
            ]
          }
        },
        "9b2a42f6c6ca4478b2e9b650286b8a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72ffa151638043c8968f3e2696291eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_95d67042dd264dae9dd275cbca18bf5d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 500,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 500,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24d4844274f644fbab449c487312baf7"
          }
        },
        "5774ffca7d86492580ac61d025d06770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ae57876caf14acc8b2f9cc369d5ef89",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 500/500 [03:04&lt;00:00,  2.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78dc0d9bbd404c17896d4c485d154747"
          }
        },
        "95d67042dd264dae9dd275cbca18bf5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24d4844274f644fbab449c487312baf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ae57876caf14acc8b2f9cc369d5ef89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78dc0d9bbd404c17896d4c485d154747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a8c4812785749308adb3c72f1437e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e2fe5943c77b436d93b860a5c18a61c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c70cc266ae04c5a946c4e71e8479172",
              "IPY_MODEL_04323ba978a3426a9381984ec6bc1ee2"
            ]
          }
        },
        "e2fe5943c77b436d93b860a5c18a61c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c70cc266ae04c5a946c4e71e8479172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3cfbf8b4c122454b8d20bf7b65424dde",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 500,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 500,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f40955cad6be45859975af2c38ace071"
          }
        },
        "04323ba978a3426a9381984ec6bc1ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83d4a8bff3b547558852d1974d8c898e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 500/500 [02:51&lt;00:00,  2.91it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed01c7ba9aab44c7b7ec4aae607460dd"
          }
        },
        "3cfbf8b4c122454b8d20bf7b65424dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f40955cad6be45859975af2c38ace071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83d4a8bff3b547558852d1974d8c898e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed01c7ba9aab44c7b7ec4aae607460dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sattwiksuman/AI_for_Trash_Collection/blob/main/Trash_Collection_RL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV8MUqYr-tHQ"
      },
      "source": [
        "#<center>Efficient Trash Collection using AI</center>\r\n",
        "---\r\n",
        "####<center>Reinforced Learning Project - Economic Modelling of Energy and Climate Systems</center>\r\n",
        "####<center>Data Analytics and Decision Science</center>\r\n",
        "####<center>RWTH Aachen Business School</center>\r\n",
        "Submitted by:<br>\r\n",
        "Priyanka Kundagol<br>\r\n",
        "Sattwik Suman Das<br>\r\n",
        "Ved Nerlikar\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3urOwETcGek"
      },
      "source": [
        "---\r\n",
        "#Introduction\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "To be added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPGv-8QnZ3Wg"
      },
      "source": [
        "###The Environment\r\n",
        "The Diagram below shows the environment in which the Agent would operate.\r\n",
        "The squares in gray are the road.\r\n",
        "The Agent is expected to collect trash from the location shown in Black boxes.\r\n",
        "The trash is actually collected from the adjacent squares marked by a dark border on the road, i.e., squares 3, 17 and 21."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDkpXo769ZQL"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZ8Ya4s_83u"
      },
      "source": [
        "###States of the Environment:\r\n",
        "The gray squares above depict the states in which the agent can reside. <br>\r\n",
        "So there are 26 states in total numbered 0 to 25. <br>\r\n",
        "The movement of the agent from one state to another is defined by defining the rewards of the adjacent states.\r\n",
        "\r\n",
        "###Actions\r\n",
        "At each state the agent can 5 actions as follows:\r\n",
        "0. Do nothing\r\n",
        "1. Move North\r\n",
        "2. Move East\r\n",
        "3. Move South\r\n",
        "4. Move West \r\n",
        "\r\n",
        "###Action_Space: control the motion\r\n",
        "For each state, the next state corresponding to the consequence of the 5 actions will be pre-defined as part of the environment definition. \r\n",
        "\r\n",
        "###Rewards\r\n",
        "The reward of the trash locations 03, 17 and 21 are defined to reflect the amount of trash in the following manner:\r\n",
        "1. if trash >= threshold:\r\n",
        "        reward= percentage of trash bin full\r\n",
        "2. else:\r\n",
        "        reward = 0\r\n",
        "<br>\r\n",
        "The threshold can be set to 70% .\r\n",
        "\r\n",
        "###Reward for road states in form of Living Penalty\r\n",
        "A Living Penalty is defined to each of the states on the road because we want the agent to move by taking the shortest time, which would ultimately be the most efficient way to travel. <br>\r\n",
        "Living Penalty is not defined to the start location '0' and the trash locations. <br>\r\n",
        "The agent is expected to reside at the starting point till it needs to make it's first trip and then reside at the location from which it has collected the trash till it needs to move to the next location to collect from the next location.<br>\r\n",
        "Living penalty can be set to -0.2\r\n",
        "\r\n",
        "###Carrying capacity of the Agent\r\n",
        "For the initial model, the agent is assumed to have infinite capacity. <br>\r\n",
        "This effectively means it never has to return to the depot to empty itself.\r\n",
        "\r\n",
        "###Timestep of the simulation\r\n",
        "The timestep can be assumed to be one hour.<br>\r\n",
        "In each timestep:\r\n",
        "1. the vehicle is assumed to have one state transition.\r\n",
        "2. the garbage is updated to new quatities\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsEbpRcKmvB8"
      },
      "source": [
        "---\r\n",
        "#Importing the packages\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THtm1DnRBmP8"
      },
      "source": [
        "import gym\r\n",
        "from gym import spaces\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from numpy.random import default_rng\r\n",
        "from random import random\r\n",
        "from random import randint\r\n",
        "import pandas as pd\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib import style\r\n",
        "style.use('ggplot')\r\n",
        "\r\n",
        "from keras.layers import Input, Dense, Dropout\r\n",
        "from keras.models import Model\r\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVSBazS5m3Ej"
      },
      "source": [
        "---\r\n",
        "#Building the Environment\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MRG3cBD3WPT"
      },
      "source": [
        "#Build the environment\r\n",
        "\r\n",
        "class TrashEnv(gym.Env):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(TrashEnv, self).__init__()\r\n",
        "        # Define action and observation space\r\n",
        "        # They must be gym.spaces objects\r\n",
        "        n_actions = 5\r\n",
        "        #self.action_space = spaces.Discrete(n_actions)\r\n",
        "\r\n",
        "        self.total_states = 17\r\n",
        "        self.observation_space = spaces.Discrete(self.total_states)\r\n",
        "\r\n",
        "        #Define the action space:\r\n",
        "        self.action_space = {}\r\n",
        "        self.action_space[0]={0:0, 1:6, 2:1, 3:0, 4:0}\r\n",
        "        self.action_space[1]={0:1, 1:1, 2:2, 3:1, 4:0}\r\n",
        "        self.action_space[2]={0:2, 1:2, 2:3, 3:2, 4:1}\r\n",
        "        self.action_space[3]={0:3, 1:3, 2:4, 3:3, 4:2}\r\n",
        "        self.action_space[4]={0:4, 1:5, 2:4, 3:4, 4:3}\r\n",
        "        self.action_space[5]={0:5, 1:9, 2:5, 3:4, 4:5}\r\n",
        "        self.action_space[6]={0:6, 1:7, 2:6, 3:0, 4:6}\r\n",
        "        self.action_space[7]={0:7, 1:11, 2:8, 3:6, 4:7}\r\n",
        "        self.action_space[8]={0:8, 1:8, 2:9, 3:8, 4:7}\r\n",
        "        self.action_space[9]={0:9, 1:10, 2:9, 3:5, 4:8}\r\n",
        "        self.action_space[10]={0:10, 1:12, 2:10, 3:9, 4:10}\r\n",
        "        self.action_space[11]={0:11, 1:16, 2:11, 3:7, 4:11}\r\n",
        "        self.action_space[12]={0:12, 1:13, 2:12, 3:10, 4:12}\r\n",
        "        self.action_space[13]={0:13, 1:13, 2:13, 3:12, 4:14}\r\n",
        "        self.action_space[14]={0:14, 1:14, 2:13, 3:14, 4:15}\r\n",
        "        self.action_space[15]={0:15, 1:15, 2:14, 3:15, 4:16}\r\n",
        "        self.action_space[16]={0:16, 1:16, 2:15, 3:11, 4:16}\r\n",
        "        \r\n",
        "        #Define the Garbage locations and corresponding initial garbage quantity values = 0:\r\n",
        "        self.garbage_quantity = {}\r\n",
        "        self.garbage_locations = [2, 10, 15]\r\n",
        "        for l in self.garbage_locations:\r\n",
        "            self.garbage_quantity[l]=0.0  \r\n",
        "\r\n",
        "        #Define maximum quatity of garbage that would be updated in one timestep:\r\n",
        "        self.max_garbage_update = 0.04   \r\n",
        "\r\n",
        "        #Define threshold after which garbage updation reflects in reward:\r\n",
        "        self.garbage_threshold = 0.7\r\n",
        "        \r\n",
        "        # Set the living penalty\r\n",
        "        self.living_penalty_base_value = -0.2\r\n",
        "        # living penalty is assigned according to the size of the state: refer to diagram attached above\r\n",
        "        self.living_penalty = {0:0,              #reward for initial location must be zero as we allow the agent to reside at 0 before it has to start\r\n",
        "                               1:2*self.living_penalty_base_value,\r\n",
        "                               2:0,              #reward for garbage location would be assigned later. it is just initialized with 0.\r\n",
        "                               3:self.living_penalty_base_value,\r\n",
        "                               4:self.living_penalty_base_value,\r\n",
        "                               5:2*self.living_penalty_base_value,\r\n",
        "                               6:2*self.living_penalty_base_value,\r\n",
        "                               7:self.living_penalty_base_value, \r\n",
        "                               8:4*self.living_penalty_base_value,\r\n",
        "                               9:self.living_penalty_base_value,\r\n",
        "                               10:0,\r\n",
        "                               11:2*self.living_penalty_base_value,\r\n",
        "                               12:self.living_penalty_base_value,\r\n",
        "                               13:self.living_penalty_base_value,\r\n",
        "                               14:3*self.living_penalty_base_value,\r\n",
        "                               15:0,\r\n",
        "                               16:self.living_penalty_base_value\r\n",
        "                               }\r\n",
        "\r\n",
        "        \r\n",
        "        #Define the Rewards:\r\n",
        "        #reward_space is defined as a dictionary of dictionaries {current_state:{action:reward}}\r\n",
        "        self.reward_space = {}\r\n",
        "        for i in range(len(self.action_space)):\r\n",
        "            self.reward_space[i] = self.update_reward(i)\r\n",
        "            '''\r\n",
        "            dummy_dict={}\r\n",
        "            for act, j in zip(self.action_space[i].keys(), self.action_space[i].values()):\r\n",
        "                dummy_dict[act]=self.living_penalty[j]\r\n",
        "                if j in self.garbage_locations:\r\n",
        "                    dummy_dict[act]=self.update_garbage_reward(j)\r\n",
        "                if act!=0 and j==i:     #larger penalty for infeasible actions\r\n",
        "                    dummy_dict[act]=2*dummy_dict[act]\r\n",
        "            self.reward_space[i]=dummy_dict\r\n",
        "            '''\r\n",
        "\r\n",
        "        # Create state attribute, initialize it in reset method\r\n",
        "        self.state = None\r\n",
        "\r\n",
        "        #Create a variable to measure the distance covered\r\n",
        "        #This will be compared against the distance covered in the naive approach\r\n",
        "        self.distance_covered = 0\r\n",
        "\r\n",
        "        #Create a variable to store the number of timesteps\r\n",
        "        #This can be used to define an epoch\r\n",
        "        self.current_time = 0\r\n",
        "\r\n",
        "        #Create a variable to set the status of training to Train or Test or None\r\n",
        "        #This allows us not to run the training everytime we run the whole notebook\r\n",
        "        self.train = None\r\n",
        "        \r\n",
        "        \r\n",
        "    def step(self, action):\r\n",
        "        \"\"\"State transition of the model.\r\n",
        "        Implements the model of the environment.\r\n",
        "        Args:\r\n",
        "            action (int): Action the agent took.\r\n",
        "        Returns:\r\n",
        "            next_state (int): The next state the environment emits.\r\n",
        "            reward (float): The reward the environment emits.\r\n",
        "            done (bool): Currently always set to 0 as we are modelling a continuous process\r\n",
        "            garbage_info (dict): Info about the rewards at the garbage locations {loc: reward}\r\n",
        "        \"\"\"\r\n",
        "        #assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\r\n",
        "        #assert action <= self.state, \"%r is to much extraction, current state: %r\" % (action, self.state)\r\n",
        "\r\n",
        "        # Calculate the next state\r\n",
        "        next_state = self.action_space[self.state[0]][action]\r\n",
        "\r\n",
        "        # Calculate the reward\r\n",
        "        ##Reward can not be set after update of the reward_space following the action\r\n",
        "        ##because the reward_space will update the garbage location to zero in case the agent arrives there giving the agent 0 reward.\r\n",
        "        #print('printing from ev class', self.reward_space[self.state[0]])\r\n",
        "        reward = self.reward_space[self.state[0]][action]\r\n",
        "\r\n",
        "        #Update the garbage_quantity and reward_space:\r\n",
        "        if next_state not in self.garbage_locations:    #in free area or stepping out of garbage location\r\n",
        "            for l in self.garbage_locations:\r\n",
        "                self.garbage_quantity[l]+=random()*self.max_garbage_update\r\n",
        "                self.reward_space[l]=self.update_reward(l)\r\n",
        "        else:\r\n",
        "            if self.state[0] not in self.garbage_locations:    #stepping into garbage location\r\n",
        "                for l in self.garbage_locations:\r\n",
        "                    if l == next_state:\r\n",
        "                        self.garbage_quantity[l]=0.0\r\n",
        "                        self.reward_space[l]=self.update_reward(l)\r\n",
        "                    else:\r\n",
        "                        self.garbage_quantity[l]+=random()*self.max_garbage_update\r\n",
        "                        self.reward_space[l]=self.update_reward(l)\r\n",
        "            else:       #waiting at a garbage location\r\n",
        "                for l in self.garbage_locations:\r\n",
        "                    self.garbage_quantity[l]+=random()*self.max_garbage_update\r\n",
        "                    self.reward_space[l]=self.update_reward(l)   \r\n",
        "\r\n",
        "        #The episode will be continuous, so there will be no 'Game Over'/ 'Done'\r\n",
        "        done = 0\r\n",
        "\r\n",
        "        self.garbage_info = {self.garbage_locations[i]: self.reward_space[l] for i, l in enumerate(self.garbage_locations)}\r\n",
        "        \r\n",
        "        #If reward of any garbage location is non zero, then living penalty at location 0 is no longer 0\r\n",
        "        #Otherwise the agent just learns to stay at the location 0 as it maximizes reward.  \r\n",
        "        if all(x==0 for x in [self.reward_space[l][0] for l in self.garbage_locations]):\r\n",
        "            self.living_penalty[0]=0\r\n",
        "        else:\r\n",
        "            self.living_penalty[0]=2*self.living_penalty_base_value\r\n",
        "        self.reward_space[0]=self.update_reward(0)\r\n",
        "\r\n",
        "        #Update the distance covered\r\n",
        "        if self.state[0] != next_state:\r\n",
        "            self.distance_covered += 1 \r\n",
        "\r\n",
        "        #Update the current time\r\n",
        "        self.current_time += 1\r\n",
        "\r\n",
        "        #Update the state to the next state:\r\n",
        "        self.state = [next_state]\r\n",
        "        for l in self.garbage_locations:\r\n",
        "            self.state.append(self.garbage_quantity[l])\r\n",
        "        #self.state = np.array(self.state)\r\n",
        "        return self.state, reward, done, self.garbage_info\r\n",
        "\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        \"\"\"Resets the environment.\r\n",
        "        Initializes the state.\r\n",
        "        Returns:\r\n",
        "            state (int): [state, garbage_quantity at garbage locations]\r\n",
        "            garbage_info (dict): Info about the rewards at the garbage locations {loc: reward}\r\n",
        "        \"\"\"\r\n",
        "        self.state = [0]\r\n",
        "        for l in self.garbage_locations:\r\n",
        "            self.garbage_quantity[l]=0.0\r\n",
        "            self.state.append(self.garbage_quantity[l])\r\n",
        "            self.reward_space[l]=self.update_garbage_reward(l)\r\n",
        "        self.distance_covered = 0\r\n",
        "        self.current_time = 0\r\n",
        "        self.garbage_info = {self.garbage_locations[i]: self.reward_space[l] for i, l in enumerate(self.garbage_locations)}\r\n",
        "        #self.state = np.array(self.state)\r\n",
        "        return self.state\r\n",
        "\r\n",
        "    #Define update reward function:\r\n",
        "    def update_reward(self, location):\r\n",
        "        i= location\r\n",
        "        dummy_dict={}\r\n",
        "        for act, j in zip(self.action_space[i].keys(), self.action_space[i].values()):\r\n",
        "            dummy_dict[act]=self.living_penalty[j]\r\n",
        "            if j in self.garbage_locations:\r\n",
        "                dummy_dict[act]=self.update_garbage_reward(j)\r\n",
        "            if act!=0 and j==i:     #larger penalty for infeasible actions\r\n",
        "                dummy_dict[act]=2*dummy_dict[act]\r\n",
        "                if i in self.garbage_locations:\r\n",
        "                    dummy_dict[act]=self.living_penalty_base_value\r\n",
        "        return dummy_dict\r\n",
        "\r\n",
        "    def update_garbage_reward(self, garbage_location):\r\n",
        "        \"\"\"Updates the reward space at a garbage location based on the quantity of garbage at the location\r\n",
        "        Returns:\r\n",
        "            reward_space[garbage_location]\r\n",
        "        \"\"\"\r\n",
        "        i=garbage_location\r\n",
        "        if i in self.garbage_locations:\r\n",
        "                if self.garbage_quantity[i] > 0.7:\r\n",
        "                    self.reward_space[i]=self.garbage_quantity[i]\r\n",
        "                else:\r\n",
        "                    self.reward_space[i]=0.0\r\n",
        "        return(self.reward_space[i])\r\n",
        "\r\n",
        "    # We will not implement render and close function\r\n",
        "    def render(self, mode='human'):\r\n",
        "        pass\r\n",
        "    def close (self):\r\n",
        "        pass"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlw4SEcadnpu",
        "outputId": "2f84539e-3658-480d-931c-1358baea926d"
      },
      "source": [
        "tr = {1:11, 2:22, 3:33}\r\n",
        "for k, v in zip(tr.keys(), tr.values()):\r\n",
        "    print(k, v)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 11\n",
            "2 22\n",
            "3 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEYibSbyduAl",
        "outputId": "d0211049-38ec-4de4-b490-38699200a911"
      },
      "source": [
        "len(tr)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8jk3YmRmaiE"
      },
      "source": [
        "---\r\n",
        "#Testing the environment\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oArN8erumbM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27979f3-1e29-4bb9-be97-30bfb1c898a9"
      },
      "source": [
        "# Create the environment\r\n",
        "test_env = TrashEnv()\r\n",
        "\r\n",
        "# Reset the environment to get initial state\r\n",
        "state = test_env.reset()\r\n",
        "state = state[0]\r\n",
        "print(test_env.garbage_info)\r\n",
        "\r\n",
        "# Create a list to store all states during the simulation\r\n",
        "columns=['action', 'state', 'reward']\r\n",
        "for key in test_env.garbage_info.keys():\r\n",
        "    columns.append(f'reward_at_garbage_{key}')\r\n",
        "for key in test_env.garbage_info.keys():\r\n",
        "    columns.append(f'garbage_quantity_at_{key}')\r\n",
        "\r\n",
        "row = {'action': 'Initialize', 'state':state, 'reward': 0.0}\r\n",
        "for key, val in zip(columns[3:6], test_env.garbage_info.values()):\r\n",
        "    row[key]=val\r\n",
        "for key, val in zip(columns[6:], test_env.garbage_quantity.values()):\r\n",
        "    row[key]=val\r\n",
        "print(row)\r\n",
        "\r\n",
        "#Pandas DataFrame test_log stores the state, action, reward and garbage parameters at each timestep. \r\n",
        "test_log = pd.DataFrame(columns=columns)\r\n",
        "test_log = test_log.append(row, ignore_index=True)  #the first row is the start position\r\n",
        "\r\n",
        "# Loop over each time step in the episode\r\n",
        "done = False\r\n",
        "for _ in range(48):\r\n",
        "    action = randint(0,4)\r\n",
        "    #print('action', action)\r\n",
        "    state, reward, _, garbage_info = test_env.step(action)\r\n",
        "    state=state[0]\r\n",
        "    #print('state', state)\r\n",
        "    row = {'action':action, 'state':state, 'reward': reward}\r\n",
        "    for key, val in zip(columns[3:], garbage_info.values()):\r\n",
        "        row[key]=val\r\n",
        "    for key, val in zip(columns[6:], test_env.garbage_quantity.values()):\r\n",
        "        row[key]=val\r\n",
        "    test_log = test_log.append(row, ignore_index=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{2: 0.0, 10: 0.0, 15: 0.0}\n",
            "{'action': 'Initialize', 'state': 0, 'reward': 0.0, 'reward_at_garbage_2': 0.0, 'reward_at_garbage_10': 0.0, 'reward_at_garbage_15': 0.0, 'garbage_quantity_at_2': 0.0, 'garbage_quantity_at_10': 0.0, 'garbage_quantity_at_15': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ClUkvP8kRL3",
        "outputId": "658540dd-1668-40e3-a429-d3682d1da76b"
      },
      "source": [
        "test_env.state"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 0.02524639513058012, 0.9059788957199079, 0.9382403449351284]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oYh3L01UCzOB",
        "outputId": "2d714e60-4db6-4f63-dd8e-168901d3485c"
      },
      "source": [
        "test_log"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>action</th>\n",
              "      <th>state</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_at_garbage_2</th>\n",
              "      <th>reward_at_garbage_10</th>\n",
              "      <th>reward_at_garbage_15</th>\n",
              "      <th>garbage_quantity_at_2</th>\n",
              "      <th>garbage_quantity_at_10</th>\n",
              "      <th>garbage_quantity_at_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Initialize</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.032546</td>\n",
              "      <td>0.021534</td>\n",
              "      <td>0.012936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.067950</td>\n",
              "      <td>0.024930</td>\n",
              "      <td>0.042775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.089047</td>\n",
              "      <td>0.045624</td>\n",
              "      <td>0.049134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.126516</td>\n",
              "      <td>0.056804</td>\n",
              "      <td>0.054016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.144496</td>\n",
              "      <td>0.084524</td>\n",
              "      <td>0.075886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.175203</td>\n",
              "      <td>0.090236</td>\n",
              "      <td>0.107178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.200109</td>\n",
              "      <td>0.116645</td>\n",
              "      <td>0.124130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.221496</td>\n",
              "      <td>0.131396</td>\n",
              "      <td>0.135787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.146289</td>\n",
              "      <td>0.153196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.017739</td>\n",
              "      <td>0.181687</td>\n",
              "      <td>0.161374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.195736</td>\n",
              "      <td>0.197003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.039943</td>\n",
              "      <td>0.209516</td>\n",
              "      <td>0.220657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.043244</td>\n",
              "      <td>0.225738</td>\n",
              "      <td>0.233999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.059618</td>\n",
              "      <td>0.236040</td>\n",
              "      <td>0.251086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.067490</td>\n",
              "      <td>0.262630</td>\n",
              "      <td>0.266031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.091816</td>\n",
              "      <td>0.301587</td>\n",
              "      <td>0.271864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.116272</td>\n",
              "      <td>0.311720</td>\n",
              "      <td>0.296676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.129172</td>\n",
              "      <td>0.322738</td>\n",
              "      <td>0.319808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.152705</td>\n",
              "      <td>0.341143</td>\n",
              "      <td>0.347892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.183111</td>\n",
              "      <td>0.366713</td>\n",
              "      <td>0.386242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.189947</td>\n",
              "      <td>0.372411</td>\n",
              "      <td>0.397948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.226652</td>\n",
              "      <td>0.378630</td>\n",
              "      <td>0.401077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.234471</td>\n",
              "      <td>0.415069</td>\n",
              "      <td>0.426729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.270931</td>\n",
              "      <td>0.421092</td>\n",
              "      <td>0.460657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.289193</td>\n",
              "      <td>0.436095</td>\n",
              "      <td>0.486619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.290864</td>\n",
              "      <td>0.445323</td>\n",
              "      <td>0.522074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.299389</td>\n",
              "      <td>0.456189</td>\n",
              "      <td>0.537445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.313276</td>\n",
              "      <td>0.458991</td>\n",
              "      <td>0.554233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.332649</td>\n",
              "      <td>0.462996</td>\n",
              "      <td>0.580186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.365133</td>\n",
              "      <td>0.471057</td>\n",
              "      <td>0.593062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.400096</td>\n",
              "      <td>0.499774</td>\n",
              "      <td>0.615255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.410521</td>\n",
              "      <td>0.535320</td>\n",
              "      <td>0.627602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.442463</td>\n",
              "      <td>0.567241</td>\n",
              "      <td>0.656102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.464148</td>\n",
              "      <td>0.598668</td>\n",
              "      <td>0.672847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.498986</td>\n",
              "      <td>0.625422</td>\n",
              "      <td>0.684668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.6000000000000001, 3: -...</td>\n",
              "      <td>0.507454</td>\n",
              "      <td>0.640222</td>\n",
              "      <td>0.695418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.2}</td>\n",
              "      <td>{0: 0.7182220739783501, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.546024</td>\n",
              "      <td>0.674325</td>\n",
              "      <td>0.718222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.7000224399553016, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.7440021737703499, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.571112</td>\n",
              "      <td>0.700022</td>\n",
              "      <td>0.744002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.7188440472236722, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.7623562680952699, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.592544</td>\n",
              "      <td>0.718844</td>\n",
              "      <td>0.762356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.7339194837259881, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.7905031029153081, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.608922</td>\n",
              "      <td>0.733919</td>\n",
              "      <td>0.790503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.7682784635433915, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.8070683524825952, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.643456</td>\n",
              "      <td>0.768278</td>\n",
              "      <td>0.807068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.777268540181616, 1: -0.2, 2: -0.2, 3: -0...</td>\n",
              "      <td>{0: 0.8235970406558987, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.679131</td>\n",
              "      <td>0.777269</td>\n",
              "      <td>0.823597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.8114312591292555, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.8437442514275334, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.687362</td>\n",
              "      <td>0.811431</td>\n",
              "      <td>0.843744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.7207786565933458, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.849161666391329, 1: -0.2, 2: -0.2, 3: -0...</td>\n",
              "      <td>{0: 0.8767812496017294, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.720779</td>\n",
              "      <td>0.849162</td>\n",
              "      <td>0.876781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.7392891225241711, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.8640327859980675, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.8788171930721661, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.739289</td>\n",
              "      <td>0.864033</td>\n",
              "      <td>0.878817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.7754903801367985, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.8821992672542441, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.8833994275537276, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.775490</td>\n",
              "      <td>0.882199</td>\n",
              "      <td>0.883399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.8955429996291457, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.9203447438701905, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.895543</td>\n",
              "      <td>0.920345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: -0.2, 3: -0.2, 4: -0.4}</td>\n",
              "      <td>{0: 0.9059788957199079, 1: -0.2, 2: -0.2, 3: -...</td>\n",
              "      <td>{0: 0.9382403449351284, 1: -0.2, 2: -0.6000000...</td>\n",
              "      <td>0.025246</td>\n",
              "      <td>0.905979</td>\n",
              "      <td>0.938240</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        action state  ...  garbage_quantity_at_10 garbage_quantity_at_15\n",
              "0   Initialize     0  ...                0.000000               0.000000\n",
              "1            1     6  ...                0.021534               0.012936\n",
              "2            3     0  ...                0.024930               0.042775\n",
              "3            3     0  ...                0.045624               0.049134\n",
              "4            0     0  ...                0.056804               0.054016\n",
              "5            2     1  ...                0.084524               0.075886\n",
              "6            1     1  ...                0.090236               0.107178\n",
              "7            3     1  ...                0.116645               0.124130\n",
              "8            0     1  ...                0.131396               0.135787\n",
              "9            2     2  ...                0.146289               0.153196\n",
              "10           2     3  ...                0.181687               0.161374\n",
              "11           4     2  ...                0.195736               0.197003\n",
              "12           3     2  ...                0.209516               0.220657\n",
              "13           4     1  ...                0.225738               0.233999\n",
              "14           4     0  ...                0.236040               0.251086\n",
              "15           1     6  ...                0.262630               0.266031\n",
              "16           0     6  ...                0.301587               0.271864\n",
              "17           2     6  ...                0.311720               0.296676\n",
              "18           2     6  ...                0.322738               0.319808\n",
              "19           0     6  ...                0.341143               0.347892\n",
              "20           2     6  ...                0.366713               0.386242\n",
              "21           3     0  ...                0.372411               0.397948\n",
              "22           1     6  ...                0.378630               0.401077\n",
              "23           2     6  ...                0.415069               0.426729\n",
              "24           1     7  ...                0.421092               0.460657\n",
              "25           3     6  ...                0.436095               0.486619\n",
              "26           4     6  ...                0.445323               0.522074\n",
              "27           2     6  ...                0.456189               0.537445\n",
              "28           4     6  ...                0.458991               0.554233\n",
              "29           4     6  ...                0.462996               0.580186\n",
              "30           3     0  ...                0.471057               0.593062\n",
              "31           1     6  ...                0.499774               0.615255\n",
              "32           2     6  ...                0.535320               0.627602\n",
              "33           2     6  ...                0.567241               0.656102\n",
              "34           4     6  ...                0.598668               0.672847\n",
              "35           3     0  ...                0.625422               0.684668\n",
              "36           0     0  ...                0.640222               0.695418\n",
              "37           1     6  ...                0.674325               0.718222\n",
              "38           0     6  ...                0.700022               0.744002\n",
              "39           2     6  ...                0.718844               0.762356\n",
              "40           4     6  ...                0.733919               0.790503\n",
              "41           2     6  ...                0.768278               0.807068\n",
              "42           4     6  ...                0.777269               0.823597\n",
              "43           3     0  ...                0.811431               0.843744\n",
              "44           2     1  ...                0.849162               0.876781\n",
              "45           4     0  ...                0.864033               0.878817\n",
              "46           2     1  ...                0.882199               0.883399\n",
              "47           2     2  ...                0.895543               0.920345\n",
              "48           3     2  ...                0.905979               0.938240\n",
              "\n",
              "[49 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prOK1irChhIY",
        "outputId": "88f6de92-34fa-49e2-d1a1-5047b0e29938"
      },
      "source": [
        "print(test_env.garbage_locations)\r\n",
        "[test_env.reward_space[l][0] for l in test_env.garbage_locations]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 10, 15]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 0.9059788957199079, 0.9382403449351284]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWXlJWVVmFx5",
        "outputId": "09b79c29-5c3c-425b-badd-5409057a0452"
      },
      "source": [
        "tt=[0, 1]\r\n",
        "all(x==0 for x in [test_env.reward_space[l][0] for l in test_env.garbage_locations])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvNPFYTMnaug",
        "outputId": "63b8587f-0c87-4606-bd93-8c4a645b2e45"
      },
      "source": [
        "test_env.reward_space[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: -0.2, 1: -0.4, 2: -0.4, 3: -0.4, 4: -0.4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBqeFbqXuPu"
      },
      "source": [
        "---\r\n",
        "#Building the Brain\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0BmFU-d3h0S"
      },
      "source": [
        "#Build the Brain\r\n",
        "'''\r\n",
        "BRAIN PARAMETERS (TBD):\r\n",
        "\r\n",
        "Input Layer: Should we just have the current state as the input or the current state\r\n",
        "            along with the garbage levels / rewards of the garbage locations as the input\r\n",
        "Output layer: output layer will have 5 nodes corresponding to the Q values\r\n",
        "            of the 5 possible actions possible for each state\r\n",
        "Dense layers: 2 dense layers with 10 nodes each #suggestion\r\n",
        "Compiler:\r\n",
        "    loss:'mse'\r\n",
        "    optimizer: 'Adam'\r\n",
        "    learning_rate= 0.001\r\n",
        "    #suggestions\r\n",
        "'''\r\n",
        "\r\n",
        "# BUILDING THE BRAIN\r\n",
        "\r\n",
        "class Brain(object):\r\n",
        "    \r\n",
        "    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\r\n",
        "    \r\n",
        "    def __init__(self, learning_rate = 0.001, number_of_state_params = 4, number_actions = 5):\r\n",
        "        \r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        \r\n",
        "        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE\r\n",
        "        states = Input(shape = (number_of_state_params,))\r\n",
        "        \r\n",
        "        # BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\r\n",
        "        x = Dense(units = 64, activation = 'sigmoid')(states)\r\n",
        "        x = Dropout(rate = 0.1)(x)\r\n",
        "        \r\n",
        "        # BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\r\n",
        "        y = Dense(units = 32, activation = 'sigmoid')(x)\r\n",
        "        y = Dropout(rate = 0.1)(y)\r\n",
        "        \r\n",
        "        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\r\n",
        "        q_values = Dense(units = number_actions, activation = 'softmax')(y)\r\n",
        "        \r\n",
        "        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\r\n",
        "        self.model = Model(inputs = states, outputs = q_values)\r\n",
        "        \r\n",
        "        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\r\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8nbqVX1X6AR"
      },
      "source": [
        "---\r\n",
        "#Creating a DQN Object (Agent)\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxUcq-GG3jeb"
      },
      "source": [
        "#Build the DQN object = Agent\r\n",
        "'''\r\n",
        "PARAMETERS:\r\n",
        "\r\n",
        "memory: list of length memory_len (we can model this as a queue)\r\n",
        "memory_max_len: length of the memory = 50 (approximately 2 days worth of timesteps)\r\n",
        "discount: 0.9\r\n",
        "'''\r\n",
        "\r\n",
        "class DQN():\r\n",
        "\r\n",
        "    def __init__(self, max_memory = 50, discount = 0.9):\r\n",
        "        self.memory = list()\r\n",
        "        self.max_memory = max_memory\r\n",
        "        self.discount = discount\r\n",
        "\r\n",
        "    '''\r\n",
        "    METHODS:\r\n",
        "\r\n",
        "    get_action:\r\n",
        "        take the current_state as input and predict the action\r\n",
        "        using epsilon delta: random or argmax(predicted Q_values for current_state)\r\n",
        "        returns: action\r\n",
        "    '''\r\n",
        "    def get_action(self, model, current_state, epsilon=0.3):\r\n",
        "        \r\n",
        "        # PLAYING THE NEXT ACTION BY EXPLORATION\r\n",
        "        if np.random.rand() <= epsilon:\r\n",
        "            action = np.random.randint(0, number_actions)\r\n",
        "        \r\n",
        "        # PLAYING THE NEXT ACTION BY INFERENCE\r\n",
        "        else:\r\n",
        "            q_values = model.predict(np.array(np.matrix(current_state)))   #current_state.shape = (4,)\r\n",
        "            action = np.argmax(q_values[0])\r\n",
        "        \r\n",
        "        return action \r\n",
        "\r\n",
        "    '''\r\n",
        "    update_memory:\r\n",
        "        get the transition made in the current timestep and append it to memory\r\n",
        "        #transition is defined by [current_state=(state + garbage_info), action, next_state, reward]\r\n",
        "        state = [state, garbage_quantity[3], garbage_quantity[7], garbage_quantity[21]]\r\n",
        "        if len of memory is == memory_max_len: pop(first element)\r\n",
        "    '''\r\n",
        "\r\n",
        "    def update_memory(self, transition):\r\n",
        "        self.memory.append(transition)\r\n",
        "        if len(self.memory) > self.max_memory:\r\n",
        "            del self.memory[0]\r\n",
        "\r\n",
        "    '''\r\n",
        "    get_batch:\r\n",
        "        batch_size: 10\r\n",
        "        input_batch = randomly select min(batch_size, len(memory)) number of states\r\n",
        "                    from the memory\r\n",
        "        target_batch = for each of the states in the input_batch, \r\n",
        "                    corresponding element of the target_batch will be a list containing\r\n",
        "                    the Q values for all the possible actions for that state\r\n",
        "                    predicted by the Brain; here we will subsequently have to update\r\n",
        "                    the Q_value corresponding to the action played (from the memory)\r\n",
        "                    to reward + discount * max(predicted values for next_state)\r\n",
        "    '''\r\n",
        "    def get_batch(self, model, batch_size = 10):\r\n",
        "        len_memory = len(self.memory)\r\n",
        "        num_inputs = len(self.memory[0][0])   #fist element of the memory = transition, first element of transition = state, input = state\r\n",
        "        num_outputs = model.output_shape[-1]\r\n",
        "        #print(num_inputs, num_outputs)\r\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))\r\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\r\n",
        "        #print(f'inputs: {inputs}, targets: {targets}')\r\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\r\n",
        "            current_state, action, next_state, reward = self.memory[idx]\r\n",
        "            inputs[i] = current_state\r\n",
        "            targets[i] = model.predict(np.matrix(current_state))[0]\r\n",
        "            #print(f'output targets of the model: {targets[i]}')\r\n",
        "            Q_sa = np.max(model.predict(np.matrix(next_state))[0])\r\n",
        "            targets[i, action] = reward + self.discount * Q_sa\r\n",
        "            #print(f'target after Q update: {targets[i]}')\r\n",
        "        return inputs, targets"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSKHFWvvYKhz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "#Training\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZVxmdiB3ni8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631,
          "referenced_widgets": [
            "eb82356fb8194b489c2a03008c8124ee",
            "9b2a42f6c6ca4478b2e9b650286b8a7f",
            "72ffa151638043c8968f3e2696291eb0",
            "5774ffca7d86492580ac61d025d06770",
            "95d67042dd264dae9dd275cbca18bf5d",
            "24d4844274f644fbab449c487312baf7",
            "4ae57876caf14acc8b2f9cc369d5ef89",
            "78dc0d9bbd404c17896d4c485d154747",
            "6a8c4812785749308adb3c72f1437e19",
            "e2fe5943c77b436d93b860a5c18a61c3",
            "9c70cc266ae04c5a946c4e71e8479172",
            "04323ba978a3426a9381984ec6bc1ee2",
            "3cfbf8b4c122454b8d20bf7b65424dde",
            "f40955cad6be45859975af2c38ace071",
            "83d4a8bff3b547558852d1974d8c898e",
            "ed01c7ba9aab44c7b7ec4aae607460dd"
          ]
        },
        "outputId": "dc544ffa-1c6b-4b17-ad94-1f95795517b2"
      },
      "source": [
        "#Train the Model and save it\r\n",
        "'''\r\n",
        "Create an object of Environment, Brain and DQN Agent\r\n",
        "create a model of brain class\r\n",
        "Number_epochs: number of epochs\r\n",
        "For each epoch:\r\n",
        "    current_state = environment.reset()\r\n",
        "    action = DQN.get_action(current_state)\r\n",
        "    next_state, reward, _, _ = environment.step(action)\r\n",
        "    update the memory: DQN.update_memory(transition=[current_state, action, next_state, reward])\r\n",
        "    get the batches of input and output:\r\n",
        "    batch_input, batch_target = DQN.get_batch()\r\n",
        "    train on the batch and update the loss:\r\n",
        "    loss += model.train_on_batch(batch_input, batch_target)\r\n",
        "    update the total reward\r\n",
        "    keep a track of the timestep\r\n",
        "    each epoch can be decided to run for a certain number of timesteps\r\n",
        "print the results of the training\r\n",
        "save the model for testing/ simulation    \r\n",
        "'''\r\n",
        "\r\n",
        "#Set the parameters:\r\n",
        "epsilon = 0.3   #for epsilon delta exploration\r\n",
        "number_actions = 5\r\n",
        "number_epochs = 2\r\n",
        "max_memory = 5\r\n",
        "discount = 0.9\r\n",
        "batch_size = 4\r\n",
        "\r\n",
        "'''\r\n",
        "Set environment parameters and include arguments in the environment class in case\r\n",
        "parameters of the environment such as:\r\n",
        "    item{garbage_update\r\n",
        "    garbage_threshold\r\n",
        "    living_penalty\r\n",
        "need to be tuned in the environment object.\r\n",
        "'''\r\n",
        "\r\n",
        "#Create the environment as an object of environmemt class\r\n",
        "env = TrashEnv()\r\n",
        "\r\n",
        "#Create the Deep Neural Network as an object of the Brain class\r\n",
        "brain = Brain()\r\n",
        "model = brain.model\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "#Create the Agent as an object of the DQN class\r\n",
        "dqn = DQN(max_memory, discount)\r\n",
        "\r\n",
        "#Set mode to training:\r\n",
        "env.train = 'Train'\r\n",
        "\r\n",
        "#Start the training\r\n",
        "if env.train == 'Train':\r\n",
        "    for epoch in range(number_epochs):\r\n",
        "        total_reward = 0.0\r\n",
        "        loss = 0.0\r\n",
        "        timestep=0\r\n",
        "        current_state = env.reset()\r\n",
        "        print(f'Epoch: {epoch+1}/{number_epochs}')\r\n",
        "        for _ in tqdm(range(500)):\r\n",
        "            #print('\\n')\r\n",
        "            action = dqn.get_action(model=model, current_state=current_state, epsilon=epsilon)\r\n",
        "            next_state, reward, _, _ = env.step(action)\r\n",
        "            transition=[current_state, action, next_state, reward]\r\n",
        "            #print(f'transition: {transition}')\r\n",
        "            dqn.update_memory(transition)\r\n",
        "            batch_input, batch_target = dqn.get_batch(model, batch_size = batch_size)\r\n",
        "            loss += model.train_on_batch(batch_input, batch_target)\r\n",
        "            total_reward +=reward\r\n",
        "            current_state = next_state\r\n",
        "        #Print the results for the current epoch\r\n",
        "        print(f'Total loss over the epoch: {loss}')\r\n",
        "        print(f'Total reward over the epoch: {total_reward}')\r\n",
        "        print(\"\\n\")\r\n",
        "        # EARLY STOPPING CAN BE IMPLEMENTED LATER\r\n",
        "        \r\n",
        "        # SAVING THE MODEL\r\n",
        "        model.save(\"model.h5\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                320       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 2,565\n",
            "Trainable params: 2,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch: 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb82356fb8194b489c2a03008c8124ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total loss over the epoch: 19.430330315575702\n",
            "Total reward over the epoch: -157.20000000000047\n",
            "\n",
            "\n",
            "Epoch: 2/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a8c4812785749308adb3c72f1437e19",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total loss over the epoch: 26.103402068139985\n",
            "Total reward over the epoch: -191.40000000000023\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsH15ZSYYQ-G"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "#Testing\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lga3zaOt3rK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a270b0-0940-4456-f8b8-0f7e78a94c86"
      },
      "source": [
        "#Test the model on a similar environment and publish the results\r\n",
        "'''\r\n",
        "build a simulation environment\r\n",
        "load the saved model\r\n",
        "current_state=environment.reset()\r\n",
        "run simulation for timestep < some value:\r\n",
        "    q_values = model.predict(current_state)\r\n",
        "    action = np.argmax(q_values)\r\n",
        "    next_state, reward, _, _ = environment.step(action)\r\n",
        "    current_state = next_state\r\n",
        "    update reward\r\n",
        "    #maintain a counter for each time the trash in a particular bin 70%:\r\n",
        "        if abs(reward[3] at current timestep - reward[3] at last timestep) \r\n",
        "'''\r\n",
        "from keras.models import load_model\r\n",
        "\r\n",
        "#Create simulation environment\r\n",
        "sim_env = TrashEnv()\r\n",
        "\r\n",
        "#Load the pre-trained model\r\n",
        "trained_model = load_model('model.h5')\r\n",
        "\r\n",
        "#Running a 5 day training:\r\n",
        "\r\n",
        "total_reward = 0.0\r\n",
        "loss = 0.0\r\n",
        "timestep=0\r\n",
        "current_state = sim_env.reset()\r\n",
        "while timestep <= 50:\r\n",
        "    #print('\\n')\r\n",
        "    #print(timestep)\r\n",
        "    q_values = trained_model.predict(np.array(np.matrix(current_state)))\r\n",
        "    #print(q_values)\r\n",
        "    action = np.argmax(q_values[0])\r\n",
        "    next_state, reward, _, _ = sim_env.step(action)\r\n",
        "    transition=[current_state, action, next_state, reward]\r\n",
        "    print(f'transition: {transition}')\r\n",
        "    total_reward +=reward\r\n",
        "    current_state = next_state\r\n",
        "    timestep +=1\r\n",
        "#Print the results for the current epoch\r\n",
        "print(\"\\n\")\r\n",
        "print(f'Total reward: {total_reward}')\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transition: [[0, 0.0, 0.0, 0.0], 0, [0, 0.03880479638630151, 0.02361615845608982, 0.03883268106481518], 0]\n",
            "transition: [[0, 0.03880479638630151, 0.02361615845608982, 0.03883268106481518], 0, [0, 0.0672915131302999, 0.06193417479179829, 0.04909165291915501], 0]\n",
            "transition: [[0, 0.0672915131302999, 0.06193417479179829, 0.04909165291915501], 0, [0, 0.10127623151954408, 0.10186985796539486, 0.08880190180468374], 0]\n",
            "transition: [[0, 0.10127623151954408, 0.10186985796539486, 0.08880190180468374], 0, [0, 0.11543981420344974, 0.13702018022702508, 0.09760448315398171], 0]\n",
            "transition: [[0, 0.11543981420344974, 0.13702018022702508, 0.09760448315398171], 0, [0, 0.14694364506261895, 0.14463706923194244, 0.11644923567417553], 0]\n",
            "transition: [[0, 0.14694364506261895, 0.14463706923194244, 0.11644923567417553], 0, [0, 0.1837281048255447, 0.15890374112667888, 0.1309173164476584], 0]\n",
            "transition: [[0, 0.1837281048255447, 0.15890374112667888, 0.1309173164476584], 0, [0, 0.1905726790391333, 0.19159590928330902, 0.14370233636182522], 0]\n",
            "transition: [[0, 0.1905726790391333, 0.19159590928330902, 0.14370233636182522], 0, [0, 0.20721840038961534, 0.20666354726741798, 0.15202404622464105], 0]\n",
            "transition: [[0, 0.20721840038961534, 0.20666354726741798, 0.15202404622464105], 0, [0, 0.23601501857366208, 0.2347397213800399, 0.15226499591089776], 0]\n",
            "transition: [[0, 0.23601501857366208, 0.2347397213800399, 0.15226499591089776], 0, [0, 0.23769612515538402, 0.25927653911606846, 0.18786745254288012], 0]\n",
            "transition: [[0, 0.23769612515538402, 0.25927653911606846, 0.18786745254288012], 0, [0, 0.26327059898343214, 0.28478392847901074, 0.2030347509577065], 0]\n",
            "transition: [[0, 0.26327059898343214, 0.28478392847901074, 0.2030347509577065], 0, [0, 0.2787635405075077, 0.29310602301269373, 0.24253388431117412], 0]\n",
            "transition: [[0, 0.2787635405075077, 0.29310602301269373, 0.24253388431117412], 0, [0, 0.2972868579378386, 0.3204775604243656, 0.27469014948152476], 0]\n",
            "transition: [[0, 0.2972868579378386, 0.3204775604243656, 0.27469014948152476], 0, [0, 0.33146279635622183, 0.3351732669973667, 0.3140119223471658], 0]\n",
            "transition: [[0, 0.33146279635622183, 0.3351732669973667, 0.3140119223471658], 0, [0, 0.33875253604455313, 0.35331456111022697, 0.3238478560011462], 0]\n",
            "transition: [[0, 0.33875253604455313, 0.35331456111022697, 0.3238478560011462], 0, [0, 0.35454504419980293, 0.3654360238718031, 0.36027206576528825], 0]\n",
            "transition: [[0, 0.35454504419980293, 0.3654360238718031, 0.36027206576528825], 0, [0, 0.3904894655616981, 0.3975116891393013, 0.39935413891404153], 0]\n",
            "transition: [[0, 0.3904894655616981, 0.3975116891393013, 0.39935413891404153], 0, [0, 0.42286912625622186, 0.4193387484795505, 0.4009760404186949], 0]\n",
            "transition: [[0, 0.42286912625622186, 0.4193387484795505, 0.4009760404186949], 0, [0, 0.43738833904772756, 0.4542813829001805, 0.41514995302555197], 0]\n",
            "transition: [[0, 0.43738833904772756, 0.4542813829001805, 0.41514995302555197], 0, [0, 0.4381945293466326, 0.4752074215814414, 0.42817818381114325], 0]\n",
            "transition: [[0, 0.4381945293466326, 0.4752074215814414, 0.42817818381114325], 0, [0, 0.47056792797454744, 0.48133398646419484, 0.4440801750236325], 0]\n",
            "transition: [[0, 0.47056792797454744, 0.48133398646419484, 0.4440801750236325], 0, [0, 0.48078383122486823, 0.514149314313117, 0.45168496126744784], 0]\n",
            "transition: [[0, 0.48078383122486823, 0.514149314313117, 0.45168496126744784], 0, [0, 0.5084512586429647, 0.5155017851920398, 0.4711865228104583], 0]\n",
            "transition: [[0, 0.5084512586429647, 0.5155017851920398, 0.4711865228104583], 0, [0, 0.5095960397393144, 0.5390210496683179, 0.48945607404814956], 0]\n",
            "transition: [[0, 0.5095960397393144, 0.5390210496683179, 0.48945607404814956], 0, [0, 0.5181197978784466, 0.5544149694054266, 0.5217423741359778], 0]\n",
            "transition: [[0, 0.5181197978784466, 0.5544149694054266, 0.5217423741359778], 0, [0, 0.5389044572836619, 0.5637264150242859, 0.5380854733625045], 0]\n",
            "transition: [[0, 0.5389044572836619, 0.5637264150242859, 0.5380854733625045], 0, [0, 0.5419524883466315, 0.583106927412033, 0.5550149811217334], 0]\n",
            "transition: [[0, 0.5419524883466315, 0.583106927412033, 0.5550149811217334], 0, [0, 0.5480147864075667, 0.5859464343821054, 0.5888789507808152], 0]\n",
            "transition: [[0, 0.5480147864075667, 0.5859464343821054, 0.5888789507808152], 0, [0, 0.5778654833245916, 0.5903175837420691, 0.6285758931011458], 0]\n",
            "transition: [[0, 0.5778654833245916, 0.5903175837420691, 0.6285758931011458], 0, [0, 0.6161955066481825, 0.6142334233309119, 0.6569053078336659], 0]\n",
            "transition: [[0, 0.6161955066481825, 0.6142334233309119, 0.6569053078336659], 0, [0, 0.6184642199014504, 0.6485273837917195, 0.6712692503297673], 0]\n",
            "transition: [[0, 0.6184642199014504, 0.6485273837917195, 0.6712692503297673], 0, [0, 0.6493358379021072, 0.6663503592212443, 0.7054500797161889], 0]\n",
            "transition: [[0, 0.6493358379021072, 0.6663503592212443, 0.7054500797161889], 0, [0, 0.6869551843607015, 0.6931716503343214, 0.725219051474913], -0.2]\n",
            "transition: [[0, 0.6869551843607015, 0.6931716503343214, 0.725219051474913], 0, [0, 0.6981502092441142, 0.7208638237333573, 0.728306091080787], -0.2]\n",
            "transition: [[0, 0.6981502092441142, 0.7208638237333573, 0.728306091080787], 0, [0, 0.7026110363100877, 0.7593816825158713, 0.7595784243130617], -0.2]\n",
            "transition: [[0, 0.7026110363100877, 0.7593816825158713, 0.7595784243130617], 0, [0, 0.7147439814972566, 0.7730346059564456, 0.7738494719707454], -0.2]\n",
            "transition: [[0, 0.7147439814972566, 0.7730346059564456, 0.7738494719707454], 0, [0, 0.7228730102990232, 0.7932877283083898, 0.7866311227553692], -0.2]\n",
            "transition: [[0, 0.7228730102990232, 0.7932877283083898, 0.7866311227553692], 0, [0, 0.7387811849013898, 0.8183969356167271, 0.8036467910113859], -0.2]\n",
            "transition: [[0, 0.7387811849013898, 0.8183969356167271, 0.8036467910113859], 0, [0, 0.7513392969057512, 0.828954876467791, 0.8201098025161253], -0.2]\n",
            "transition: [[0, 0.7513392969057512, 0.828954876467791, 0.8201098025161253], 0, [0, 0.7680788063746204, 0.8438110485871457, 0.8475898788771289], -0.2]\n",
            "transition: [[0, 0.7680788063746204, 0.8438110485871457, 0.8475898788771289], 0, [0, 0.7808337511875407, 0.8689507294193544, 0.8650295405508678], -0.2]\n",
            "transition: [[0, 0.7808337511875407, 0.8689507294193544, 0.8650295405508678], 0, [0, 0.7928873120377073, 0.8749732327162492, 0.8942245465005022], -0.2]\n",
            "transition: [[0, 0.7928873120377073, 0.8749732327162492, 0.8942245465005022], 0, [0, 0.8073586999609507, 0.8916870321153016, 0.8994615758451123], -0.2]\n",
            "transition: [[0, 0.8073586999609507, 0.8916870321153016, 0.8994615758451123], 0, [0, 0.832980202558688, 0.911457346297311, 0.9344242698177065], -0.2]\n",
            "transition: [[0, 0.832980202558688, 0.911457346297311, 0.9344242698177065], 0, [0, 0.8542488956021537, 0.9485080704251262, 0.9667146492293086], -0.2]\n",
            "transition: [[0, 0.8542488956021537, 0.9485080704251262, 0.9667146492293086], 0, [0, 0.8847562524208911, 0.9651260311924483, 0.9814918071863241], -0.2]\n",
            "transition: [[0, 0.8847562524208911, 0.9651260311924483, 0.9814918071863241], 0, [0, 0.8999101375965569, 0.9807404362415056, 0.997625411052488], -0.2]\n",
            "transition: [[0, 0.8999101375965569, 0.9807404362415056, 0.997625411052488], 0, [0, 0.9215724605572825, 1.0196890771515101, 1.0228437306957603], -0.2]\n",
            "transition: [[0, 0.9215724605572825, 1.0196890771515101, 1.0228437306957603], 0, [0, 0.9570741139234338, 1.0553288978189064, 1.023076864349169], -0.2]\n",
            "transition: [[0, 0.9570741139234338, 1.0553288978189064, 1.023076864349169], 0, [0, 0.9598579444007563, 1.0951635231817836, 1.0515285908333256], -0.2]\n",
            "transition: [[0, 0.9598579444007563, 1.0951635231817836, 1.0515285908333256], 0, [0, 0.9700994821558979, 1.1317171740720395, 1.0678581346031908], -0.2]\n",
            "\n",
            "\n",
            "Total reward: -3.800000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqqBReAPGtOB",
        "outputId": "9540467e-2c62-4f13-ab60-402502dd43e4"
      },
      "source": [
        "np.argmax(q_values[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiIxyQvNZGtS"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "#Printing and Visualizing the Results\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu5_cz_ZZNB3"
      },
      "source": [
        "#Print the results\r\n",
        "#Report the observations\r\n",
        "#Create a visualization\r\n",
        "#Write a Conclusion"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-BF4RSdfOdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "fe15a628-5c0e-413c-8e96-7b5aa61f8632"
      },
      "source": [
        "#Animation of the results\r\n",
        "\r\n",
        "#convert results in desired format (TBD)\r\n",
        "results = []\r\n",
        "'''\r\n",
        "#Making the grid\r\n",
        "N = 6\r\n",
        "M = 7\r\n",
        "Roads = np.ones((N, M)) * np.nan    #Empty set\r\n",
        "fig, ax = plt.subplots(1, 1, tight_layout=True)  #fig + axes\r\n",
        "Roads_cmap = matplotlib.colors.ListedColormap(['Grey']) #grey for borders\r\n",
        "for x in range(N + 1):      #draw grid\r\n",
        "    for y in range(M+1):\r\n",
        "        ax.axhline(x, lw=2, color='k', zorder=5)\r\n",
        "        ax.axvline(x, lw=2, color='k', zorder=5)\r\n",
        "ax.imshow(Roads, interpolation='none', cmap=Roads_cmap, extent=[0, N, 0, M], zorder=0)   #roads\r\n",
        "ax.axis('off')   #remove axis\r\n",
        "\r\n",
        "#Highlighting pre-defined cells (Roads & pickups)\r\n",
        "\r\n",
        "#Creating animation\r\n",
        "from matplotlib.animation import FuncAnimation\r\n",
        "\r\n",
        "fig, ax = plt.subplots()\r\n",
        "xdata, ydata = [], []\r\n",
        "ln, = plt.plot([], [], 'ro')\r\n",
        "\r\n",
        "def init():\r\n",
        "    ax.set_xlim(0, 2*np.pi)\r\n",
        "    ax.set_ylim(-1, 1)\r\n",
        "    return ln,\r\n",
        "\r\n",
        "def update(frame):\r\n",
        "    xdata.append(frame)\r\n",
        "    ydata.append(np.sin(frame))\r\n",
        "    ln.set_data(xdata, ydata)\r\n",
        "    return ln,\r\n",
        "\r\n",
        "ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 128),\r\n",
        "                    init_func=init, blit=True)\r\n",
        "plt.show()\r\n",
        "'''"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#Making the grid\\nN = 6\\nM = 7\\nRoads = np.ones((N, M)) * np.nan    #Empty set\\nfig, ax = plt.subplots(1, 1, tight_layout=True)  #fig + axes\\nRoads_cmap = matplotlib.colors.ListedColormap(['Grey']) #grey for borders\\nfor x in range(N + 1):      #draw grid\\n    for y in range(M+1):\\n        ax.axhline(x, lw=2, color='k', zorder=5)\\n        ax.axvline(x, lw=2, color='k', zorder=5)\\nax.imshow(Roads, interpolation='none', cmap=Roads_cmap, extent=[0, N, 0, M], zorder=0)   #roads\\nax.axis('off')   #remove axis\\n\\n#Highlighting pre-defined cells (Roads & pickups)\\n\\n#Creating animation\\nfrom matplotlib.animation import FuncAnimation\\n\\nfig, ax = plt.subplots()\\nxdata, ydata = [], []\\nln, = plt.plot([], [], 'ro')\\n\\ndef init():\\n    ax.set_xlim(0, 2*np.pi)\\n    ax.set_ylim(-1, 1)\\n    return ln,\\n\\ndef update(frame):\\n    xdata.append(frame)\\n    ydata.append(np.sin(frame))\\n    ln.set_data(xdata, ydata)\\n    return ln,\\n\\nani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 128),\\n                    init_func=init, blit=True)\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}