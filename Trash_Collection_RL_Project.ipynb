{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trash_Collection_RL_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sattwiksuman/AI_for_Trash_Collection/blob/grouped_environment/Trash_Collection_RL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV8MUqYr-tHQ"
      },
      "source": [
        "#<center>Efficient Trash Collection using AI</center>\r\n",
        "---\r\n",
        "####<center>Reinforced Learning Project - Economic Modelling of Energy and Climate Systems</center>\r\n",
        "####<center>Data Analytics and Decision Science</center>\r\n",
        "####<center>RWTH Aachen Business School</center>\r\n",
        "Submitted by:<br>\r\n",
        "Priyanka Kundagol<br>\r\n",
        "Sattwik Suman Das<br>\r\n",
        "Ved Nerlikar\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3urOwETcGek"
      },
      "source": [
        "---\r\n",
        "#Introduction\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "To be added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPGv-8QnZ3Wg"
      },
      "source": [
        "###The Environment\r\n",
        "The Diagram below shows the environment in which the Agent would operate.\r\n",
        "The squares in gray are the road.\r\n",
        "The Agent is expected to collect trash from the location shown in Black boxes.\r\n",
        "The trash is actually collected from the adjacent squares marked by a dark border on the road, i.e., squares 3, 17 and 21."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDkpXo769ZQL"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZ8Ya4s_83u"
      },
      "source": [
        "###States of the Environment:\r\n",
        "The gray squares above depict the states in which the agent can reside. <br>\r\n",
        "So there are 26 states in total numbered 0 to 25. <br>\r\n",
        "The movement of the agent from one state to another is defined by defining the rewards of the adjacent states.\r\n",
        "\r\n",
        "###Actions\r\n",
        "At each state the agent can 5 actions as follows:\r\n",
        "0. Do nothing\r\n",
        "1. Move North\r\n",
        "2. Move East\r\n",
        "3. Move South\r\n",
        "4. Move West \r\n",
        "\r\n",
        "###Action_Space: control the motion\r\n",
        "For each state, the next state corresponding to the consequence of the 5 actions will be pre-defined as part of the environment definition. \r\n",
        "\r\n",
        "###Rewards\r\n",
        "The reward of the trash locations 03, 17 and 21 are defined to reflect the amount of trash in the following manner:\r\n",
        "1. if trash >= threshold:\r\n",
        "        reward= percentage of trash bin full\r\n",
        "2. else:\r\n",
        "        reward = 0\r\n",
        "<br>\r\n",
        "The threshold can be set to 70% .\r\n",
        "\r\n",
        "###Reward for road states in form of Living Penalty\r\n",
        "A Living Penalty is defined to each of the states on the road because we want the agent to move by taking the shortest time, which would ultimately be the most efficient way to travel. <br>\r\n",
        "Living Penalty is not defined to the start location '0' and the trash locations. <br>\r\n",
        "The agent is expected to reside at the starting point till it needs to make it's first trip and then reside at the location from which it has collected the trash till it needs to move to the next location to collect from the next location.<br>\r\n",
        "Living penalty can be set to -0.2\r\n",
        "\r\n",
        "###Carrying capacity of the Agent\r\n",
        "For the initial model, the agent is assumed to have infinite capacity. <br>\r\n",
        "This effectively means it never has to return to the depot to empty itself.\r\n",
        "\r\n",
        "###Timestep of the simulation\r\n",
        "The timestep can be assumed to be one hour.<br>\r\n",
        "In each timestep:\r\n",
        "1. the vehicle is assumed to have one state transition.\r\n",
        "2. the garbage is updated to new quatities\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsEbpRcKmvB8"
      },
      "source": [
        "---\r\n",
        "#Importing the packages\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THtm1DnRBmP8"
      },
      "source": [
        "import gym\r\n",
        "from gym import spaces\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from numpy.random import default_rng\r\n",
        "from random import random\r\n",
        "from random import randint\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib import style\r\n",
        "style.use('ggplot')\r\n",
        "\r\n",
        "from keras.layers import Input, Dense, Dropout\r\n",
        "from keras.models import Model\r\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVSBazS5m3Ej"
      },
      "source": [
        "---\r\n",
        "#Building the Environment\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MRG3cBD3WPT"
      },
      "source": [
        "#Build the environment\r\n",
        "\r\n",
        "class TrashEnv(gym.Env):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(TrashEnv, self).__init__()\r\n",
        "        # Define action and observation space\r\n",
        "        # They must be gym.spaces objects\r\n",
        "        n_actions = 5\r\n",
        "        #self.action_space = spaces.Discrete(n_actions)\r\n",
        "\r\n",
        "        self.total_states = 17\r\n",
        "        self.observation_space = spaces.Discrete(self.total_states)\r\n",
        "\r\n",
        "        #Define the action space:\r\n",
        "        self.action_space = {}\r\n",
        "        self.action_space[0]={0:0, 1:6, 2:1, 3:0, 4:0}\r\n",
        "        self.action_space[1]={0:1, 1:1, 2:2, 3:1, 4:0}\r\n",
        "        self.action_space[2]={0:2, 1:2, 2:3, 3:2, 4:1}\r\n",
        "        self.action_space[3]={0:3, 1:3, 2:4, 3:3, 4:2}\r\n",
        "        self.action_space[4]={0:4, 1:5, 2:4, 3:4, 4:3}\r\n",
        "        self.action_space[5]={0:5, 1:9, 2:5, 3:4, 4:5}\r\n",
        "        self.action_space[6]={0:6, 1:7, 2:6, 3:0, 4:6}\r\n",
        "        self.action_space[7]={0:7, 1:11, 2:8, 3:6, 4:7}\r\n",
        "        self.action_space[8]={0:8, 1:8, 2:9, 3:8, 4:7}\r\n",
        "        self.action_space[9]={0:9, 1:10, 2:9, 3:5, 4:8}\r\n",
        "        self.action_space[10]={0:10, 1:12, 2:10, 3:9, 4:10}\r\n",
        "        self.action_space[11]={0:11, 1:16, 2:11, 3:7, 4:11}\r\n",
        "        self.action_space[12]={0:12, 1:13, 2:12, 3:10, 4:12}\r\n",
        "        self.action_space[13]={0:13, 1:13, 2:13, 3:12, 4:14}\r\n",
        "        self.action_space[14]={0:14, 1:14, 2:13, 3:14, 4:15}\r\n",
        "        self.action_space[15]={0:15, 1:15, 2:14, 3:15, 4:16}\r\n",
        "        self.action_space[16]={0:16, 1:16, 2:15, 3:11, 4:16}\r\n",
        "        \r\n",
        "        #Define the Garbage locations and corresponding initial garbage quantity values = 0:\r\n",
        "        self.garbage_quantity = {}\r\n",
        "        self.garbage_locations = [2, 10, 15]\r\n",
        "        for l in self.garbage_locations:\r\n",
        "            self.garbage_quantity[l]=0.0  \r\n",
        "\r\n",
        "        #Define maximum quatity of garbage that would be updated in one timestep:\r\n",
        "        self.max_garbage_update = 0.04   \r\n",
        "\r\n",
        "        #Define threshold after which garbage updation reflects in reward:\r\n",
        "        self.garbage_threshold = 0.7\r\n",
        "        \r\n",
        "        # Set the living penalty\r\n",
        "        self.living_penalty_base_value = -0.2\r\n",
        "        # living penalty is assigned according to the size of the state: refer to diagram attached above\r\n",
        "        self.living_penalty = {0:0,              #reward for initial location must be zero as we allow the agent to reside at 0 before it has to start\r\n",
        "                               1:2*self.living_penalty_base_value,\r\n",
        "                               2:0,              #reward for garbage location would be assigned later. it is just initialized with 0.\r\n",
        "                               3:self.living_penalty_base_value,\r\n",
        "                               4:self.living_penalty_base_value,\r\n",
        "                               5:2*self.living_penalty_base_value,\r\n",
        "                               6:2*self.living_penalty_base_value,\r\n",
        "                               7:self.living_penalty_base_value, \r\n",
        "                               8:4*self.living_penalty_base_value,\r\n",
        "                               9:self.living_penalty_base_value,\r\n",
        "                               10:0,\r\n",
        "                               11:2*self.living_penalty_base_value,\r\n",
        "                               12:self.living_penalty_base_value,\r\n",
        "                               13:self.living_penalty_base_value,\r\n",
        "                               14:3*self.living_penalty_base_value,\r\n",
        "                               15:0,\r\n",
        "                               16:self.living_penalty_base_value\r\n",
        "                               }\r\n",
        "\r\n",
        "        \r\n",
        "        #Define the Rewards:\r\n",
        "        #reward_space is defined as a dictionary of dictionaries {current_state:{action:reward}}\r\n",
        "        self.reward_space = {}\r\n",
        "        for i in range(len(self.action_space)):\r\n",
        "            self.reward_space[i] = self.update_reward(i)\r\n",
        "            '''\r\n",
        "            dummy_dict={}\r\n",
        "            for act, j in zip(self.action_space[i].keys(), self.action_space[i].values()):\r\n",
        "                dummy_dict[act]=self.living_penalty[j]\r\n",
        "                if j in self.garbage_locations:\r\n",
        "                    dummy_dict[act]=self.update_garbage_reward(j)\r\n",
        "                if act!=0 and j==i:     #larger penalty for infeasible actions\r\n",
        "                    dummy_dict[act]=2*dummy_dict[act]\r\n",
        "            self.reward_space[i]=dummy_dict\r\n",
        "            '''\r\n",
        "\r\n",
        "        # Create state attribute, initialize it in reset method\r\n",
        "        self.state = None\r\n",
        "\r\n",
        "        #Create a variable to measure the distance covered\r\n",
        "        #This will be compared against the distance covered in the naive approach\r\n",
        "        self.distance_covered = 0\r\n",
        "\r\n",
        "        #Create a variable to store the number of timesteps\r\n",
        "        #This can be used to define an epoch\r\n",
        "        self.current_time = 0\r\n",
        "\r\n",
        "        #Create a variable to set the status of training to Train or Test or None\r\n",
        "        #This allows us not to run the training everytime we run the whole notebook\r\n",
        "        self.train = None\r\n",
        "        \r\n",
        "        \r\n",
        "    def step(self, action):\r\n",
        "        \"\"\"State transition of the model.\r\n",
        "        Implements the model of the environment.\r\n",
        "        Args:\r\n",
        "            action (int): Action the agent took.\r\n",
        "        Returns:\r\n",
        "            next_state (int): The next state the environment emits.\r\n",
        "            reward (float): The reward the environment emits.\r\n",
        "            done (bool): Currently always set to 0 as we are modelling a continuous process\r\n",
        "            garbage_info (dict): Info about the rewards at the garbage locations {loc: reward}\r\n",
        "        \"\"\"\r\n",
        "        #assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\r\n",
        "        #assert action <= self.state, \"%r is to much extraction, current state: %r\" % (action, self.state)\r\n",
        "\r\n",
        "        # Calculate the next state\r\n",
        "        next_state = self.action_space[self.state[0]][action]\r\n",
        "\r\n",
        "        # Calculate the reward\r\n",
        "        ##Reward can not be set after update of the reward_space following the action\r\n",
        "        ##because the reward_space will update the garbage location to zero in case the agent arrives there giving the agent 0 reward.\r\n",
        "        #print('printing from ev class', self.reward_space[self.state[0]])\r\n",
        "        reward = self.reward_space[self.state[0]][action]\r\n",
        "\r\n",
        "        #Update the garbage_quantity and reward_space:\r\n",
        "        if next_state not in self.garbage_locations:    #in free area or stepping out of garbage location\r\n",
        "            for l in self.garbage_locations:\r\n",
        "                self.garbage_quantity[l]+=random()*self.max_garbage_update\r\n",
        "                self.reward_space[l]=self.update_reward(l)\r\n",
        "        else:\r\n",
        "            if self.state[0] not in self.garbage_locations:    #stepping into garbage location\r\n",
        "                for l in self.garbage_locations:\r\n",
        "                    if l == next_state:\r\n",
        "                        self.garbage_quantity[l]=0.0\r\n",
        "                        self.reward_space[l]=self.update_reward(l)\r\n",
        "                    else:\r\n",
        "                        self.garbage_quantity[l]+=random()*self.max_garbage_update\r\n",
        "                        self.reward_space[l]=self.update_reward(l)\r\n",
        "            else:       #waiting at a garbage location\r\n",
        "                for l in self.garbage_locations:\r\n",
        "                    self.garbage_quantity[l]+=random()*self.max_garbage_update\r\n",
        "                    self.reward_space[l]=self.update_reward(l)   \r\n",
        "\r\n",
        "        #The episode will be continuous, so there will be no 'Game Over'/ 'Done'\r\n",
        "        done = 0\r\n",
        "\r\n",
        "        self.garbage_info = {self.garbage_locations[i]: self.reward_space[l] for i, l in enumerate(self.garbage_locations)}\r\n",
        "\r\n",
        "        #Update the distance covered\r\n",
        "        if self.state[0] != next_state:\r\n",
        "            self.distance_covered += 1 \r\n",
        "\r\n",
        "        #Update the current time\r\n",
        "        self.current_time += 1\r\n",
        "\r\n",
        "        #Update the state to the next state:\r\n",
        "        self.state = [next_state]\r\n",
        "        for l in self.garbage_locations:\r\n",
        "            self.state.append(self.garbage_quantity[l])\r\n",
        "\r\n",
        "        return self.state, reward, done, self.garbage_info\r\n",
        "\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        \"\"\"Resets the environment.\r\n",
        "        Initializes the state.\r\n",
        "        Returns:\r\n",
        "            state (int): [state, garbage_quantity at garbage locations]\r\n",
        "            garbage_info (dict): Info about the rewards at the garbage locations {loc: reward}\r\n",
        "        \"\"\"\r\n",
        "        self.state = [0]\r\n",
        "        for l in self.garbage_locations:\r\n",
        "            self.garbage_quantity[l]=0.0\r\n",
        "            self.state.append(self.garbage_quantity[l])\r\n",
        "            self.reward_space[l]=self.update_garbage_reward(l)\r\n",
        "        self.distance_covered = 0\r\n",
        "        self.current_time = 0\r\n",
        "        self.garbage_info = {self.garbage_locations[i]: self.reward_space[l] for i, l in enumerate(self.garbage_locations)}\r\n",
        "        \r\n",
        "        return self.state\r\n",
        "\r\n",
        "    #Define update reward function:\r\n",
        "    def update_reward(self, location):\r\n",
        "        i= location\r\n",
        "        dummy_dict={}\r\n",
        "        for act, j in zip(self.action_space[i].keys(), self.action_space[i].values()):\r\n",
        "            dummy_dict[act]=self.living_penalty[j]\r\n",
        "            if j in self.garbage_locations:\r\n",
        "                dummy_dict[act]=self.update_garbage_reward(j)\r\n",
        "            if act!=0 and j==i:     #larger penalty for infeasible actions\r\n",
        "                dummy_dict[act]=2*dummy_dict[act]\r\n",
        "        return dummy_dict\r\n",
        "\r\n",
        "    def update_garbage_reward(self, garbage_location):\r\n",
        "        \"\"\"Updates the reward space at a garbage location based on the quantity of garbage at the location\r\n",
        "        Returns:\r\n",
        "            reward_space[garbage_location]\r\n",
        "        \"\"\"\r\n",
        "        i=garbage_location\r\n",
        "        if i in self.garbage_locations:\r\n",
        "                if self.garbage_quantity[i] > 0.7:\r\n",
        "                    self.reward_space[i]=self.garbage_quantity[i]\r\n",
        "                else:\r\n",
        "                    self.reward_space[i]=0.0\r\n",
        "        return(self.reward_space[i])\r\n",
        "\r\n",
        "    # We will not implement render and close function\r\n",
        "    def render(self, mode='human'):\r\n",
        "        pass\r\n",
        "    def close (self):\r\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlw4SEcadnpu",
        "outputId": "0fe276df-dbcb-4905-b001-ed8bd37e8d0b"
      },
      "source": [
        "tr = {1:11, 2:22, 3:33}\r\n",
        "for k, v in zip(tr.keys(), tr.values()):\r\n",
        "    print(k, v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 11\n",
            "2 22\n",
            "3 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEYibSbyduAl",
        "outputId": "ff9b2df6-33b7-4889-9e94-d707754e2a64"
      },
      "source": [
        "len(tr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8jk3YmRmaiE"
      },
      "source": [
        "---\r\n",
        "#Testing the environment\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oArN8erumbM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0379ec2f-9f51-4b91-8da9-bd55d6554c2c"
      },
      "source": [
        "# Create the environment\r\n",
        "test_env = TrashEnv()\r\n",
        "\r\n",
        "# Reset the environment to get initial state\r\n",
        "state = test_env.reset()\r\n",
        "state = state[0]\r\n",
        "print(test_env.garbage_info)\r\n",
        "\r\n",
        "# Create a list to store all states during the simulation\r\n",
        "columns=['action', 'state', 'reward']\r\n",
        "for key in test_env.garbage_info.keys():\r\n",
        "    columns.append(f'reward_at_garbage_{key}')\r\n",
        "for key in test_env.garbage_info.keys():\r\n",
        "    columns.append(f'garbage_quantity_at_{key}')\r\n",
        "\r\n",
        "row = {'action': 'Initialize', 'state':state, 'reward': 0.0}\r\n",
        "for key, val in zip(columns[3:6], test_env.garbage_info.values()):\r\n",
        "    row[key]=val\r\n",
        "for key, val in zip(columns[6:], test_env.garbage_quantity.values()):\r\n",
        "    row[key]=val\r\n",
        "print(row)\r\n",
        "\r\n",
        "#Pandas DataFrame test_log stores the state, action, reward and garbage parameters at each timestep. \r\n",
        "test_log = pd.DataFrame(columns=columns)\r\n",
        "test_log = test_log.append(row, ignore_index=True)  #the first row is the start position\r\n",
        "\r\n",
        "# Loop over each time step in the episode\r\n",
        "done = False\r\n",
        "for _ in range(48):\r\n",
        "    action = randint(0,4)\r\n",
        "    #print('action', action)\r\n",
        "    state, reward, _, garbage_info = test_env.step(action)\r\n",
        "    state=state[0]\r\n",
        "    #print('state', state)\r\n",
        "    row = {'action':action, 'state':state, 'reward': reward}\r\n",
        "    for key, val in zip(columns[3:], garbage_info.values()):\r\n",
        "        row[key]=val\r\n",
        "    for key, val in zip(columns[6:], test_env.garbage_quantity.values()):\r\n",
        "        row[key]=val\r\n",
        "    test_log = test_log.append(row, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{2: 0.0, 10: 0.0, 15: 0.0}\n",
            "{'action': 'Initialize', 'state': 0, 'reward': 0.0, 'reward_at_garbage_2': 0.0, 'reward_at_garbage_10': 0.0, 'reward_at_garbage_15': 0.0, 'garbage_quantity_at_2': 0.0, 'garbage_quantity_at_10': 0.0, 'garbage_quantity_at_15': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ClUkvP8kRL3",
        "outputId": "69f87e91-aa4a-4005-f000-8a81e2e81311"
      },
      "source": [
        "test_env.state"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 0.0, 0.07477718669230474, 0.06886781693907587]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oYh3L01UCzOB",
        "outputId": "d537735e-17a5-46b5-f3b9-109670f35c81"
      },
      "source": [
        "test_log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>action</th>\n",
              "      <th>state</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_at_garbage_2</th>\n",
              "      <th>reward_at_garbage_10</th>\n",
              "      <th>reward_at_garbage_15</th>\n",
              "      <th>garbage_quantity_at_2</th>\n",
              "      <th>garbage_quantity_at_10</th>\n",
              "      <th>garbage_quantity_at_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Initialize</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.008599</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>0.029880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.046298</td>\n",
              "      <td>0.038436</td>\n",
              "      <td>0.032542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.062213</td>\n",
              "      <td>0.049084</td>\n",
              "      <td>0.050034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.101215</td>\n",
              "      <td>0.082868</td>\n",
              "      <td>0.079886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.138309</td>\n",
              "      <td>0.116132</td>\n",
              "      <td>0.099835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.157930</td>\n",
              "      <td>0.148278</td>\n",
              "      <td>0.131868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.183454</td>\n",
              "      <td>0.150229</td>\n",
              "      <td>0.165785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.203848</td>\n",
              "      <td>0.169476</td>\n",
              "      <td>0.176049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.210756</td>\n",
              "      <td>0.171187</td>\n",
              "      <td>0.210649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.249052</td>\n",
              "      <td>0.187061</td>\n",
              "      <td>0.243643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.251710</td>\n",
              "      <td>0.205541</td>\n",
              "      <td>0.249853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.271653</td>\n",
              "      <td>0.236491</td>\n",
              "      <td>0.272505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272436</td>\n",
              "      <td>0.311865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.006846</td>\n",
              "      <td>0.287300</td>\n",
              "      <td>0.323227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.016247</td>\n",
              "      <td>0.294672</td>\n",
              "      <td>0.323571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.032091</td>\n",
              "      <td>0.323097</td>\n",
              "      <td>0.352002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.332318</td>\n",
              "      <td>0.379297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.005802</td>\n",
              "      <td>0.360128</td>\n",
              "      <td>0.413131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.029948</td>\n",
              "      <td>0.381079</td>\n",
              "      <td>0.435541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.041260</td>\n",
              "      <td>0.408303</td>\n",
              "      <td>0.451716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.046759</td>\n",
              "      <td>0.425891</td>\n",
              "      <td>0.490626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.070023</td>\n",
              "      <td>0.426210</td>\n",
              "      <td>0.519857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.074340</td>\n",
              "      <td>0.460061</td>\n",
              "      <td>0.559243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.471168</td>\n",
              "      <td>0.568596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.005489</td>\n",
              "      <td>0.494598</td>\n",
              "      <td>0.577183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.019842</td>\n",
              "      <td>0.532968</td>\n",
              "      <td>0.617115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.039484</td>\n",
              "      <td>0.554329</td>\n",
              "      <td>0.619317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.047175</td>\n",
              "      <td>0.586947</td>\n",
              "      <td>0.655698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.078135</td>\n",
              "      <td>0.601921</td>\n",
              "      <td>0.684064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.6000000000000001, 3: 0....</td>\n",
              "      <td>0.097036</td>\n",
              "      <td>0.630380</td>\n",
              "      <td>0.689188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.7086538508676318, 1: 1.4173077017352635,...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.642488</td>\n",
              "      <td>0.708654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.746915774235327, 1: 1.493831548470654, 2...</td>\n",
              "      <td>0.031460</td>\n",
              "      <td>0.643221</td>\n",
              "      <td>0.746916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.0, 1: -0.2, 2: 0.0, 3: -0.2, 4: 0.0}</td>\n",
              "      <td>{0: 0.7675199567444112, 1: 1.5350399134888224,...</td>\n",
              "      <td>0.050703</td>\n",
              "      <td>0.681302</td>\n",
              "      <td>0.767520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.7029672790859817, 1: -0.2, 2: 1.40593455...</td>\n",
              "      <td>{0: 0.772535790868668, 1: 1.545071581737336, 2...</td>\n",
              "      <td>0.069783</td>\n",
              "      <td>0.702967</td>\n",
              "      <td>0.772536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.7271320434409416, 1: -0.2, 2: 1.45426408...</td>\n",
              "      <td>{0: 0.7768625227182887, 1: 1.5537250454365774,...</td>\n",
              "      <td>0.086983</td>\n",
              "      <td>0.727132</td>\n",
              "      <td>0.776863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.7522781585228008, 1: -0.2, 2: 1.50455631...</td>\n",
              "      <td>{0: 0.8045610476895978, 1: 1.6091220953791956,...</td>\n",
              "      <td>0.119365</td>\n",
              "      <td>0.752278</td>\n",
              "      <td>0.804561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.7643566381766284, 1: -0.2, 2: 1.52871327...</td>\n",
              "      <td>{0: 0.8100020478556951, 1: 1.6200040957113901,...</td>\n",
              "      <td>0.153806</td>\n",
              "      <td>0.764357</td>\n",
              "      <td>0.810002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.7879152686534532, 1: -0.2, 2: 1.57583053...</td>\n",
              "      <td>{0: 0.8115726183844116, 1: 1.6231452367688233,...</td>\n",
              "      <td>0.169207</td>\n",
              "      <td>0.787915</td>\n",
              "      <td>0.811573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.7892713737070259, 1: -0.2, 2: 1.57854274...</td>\n",
              "      <td>{0: 0.8392507716551505, 1: 1.678501543310301, ...</td>\n",
              "      <td>0.199769</td>\n",
              "      <td>0.789271</td>\n",
              "      <td>0.839251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.7905627101792078, 1: -0.2, 2: 1.58112542...</td>\n",
              "      <td>{0: 0.855562584879456, 1: 1.711125169758912, 2...</td>\n",
              "      <td>0.209416</td>\n",
              "      <td>0.790563</td>\n",
              "      <td>0.855563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.8116819765691571, 1: -0.2, 2: 1.62336395...</td>\n",
              "      <td>{0: 0.861639880816065, 1: 1.72327976163213, 2:...</td>\n",
              "      <td>0.236279</td>\n",
              "      <td>0.811682</td>\n",
              "      <td>0.861640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.8264484655875831, 1: -0.2, 2: 1.65289693...</td>\n",
              "      <td>{0: 0.8919542607853251, 1: 1.7839085215706503,...</td>\n",
              "      <td>0.255824</td>\n",
              "      <td>0.826448</td>\n",
              "      <td>0.891954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.8562170725588995, 1: -0.2, 2: 1.71243414...</td>\n",
              "      <td>{0: 0.9175016332175677, 1: 1.8350032664351354,...</td>\n",
              "      <td>0.261268</td>\n",
              "      <td>0.856217</td>\n",
              "      <td>0.917502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.8596359019166485, 1: -0.2, 2: 1.71927180...</td>\n",
              "      <td>{0: 0.9271912721233087, 1: 1.8543825442466173,...</td>\n",
              "      <td>0.277292</td>\n",
              "      <td>0.859636</td>\n",
              "      <td>0.927191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.8751318229368956, 1: -0.2, 2: 1.75026364...</td>\n",
              "      <td>{0: 0.9393349050931189, 1: 1.8786698101862378,...</td>\n",
              "      <td>0.312215</td>\n",
              "      <td>0.875132</td>\n",
              "      <td>0.939335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.8926142991199303, 1: -0.2, 2: 1.78522859...</td>\n",
              "      <td>{0: 0.9679818866424157, 1: 1.9359637732848314,...</td>\n",
              "      <td>0.338539</td>\n",
              "      <td>0.892614</td>\n",
              "      <td>0.967982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.9290054140558611, 1: -0.2, 2: 1.85801082...</td>\n",
              "      <td>{0: 0.9748115378285563, 1: 1.9496230756571127,...</td>\n",
              "      <td>0.374995</td>\n",
              "      <td>0.929005</td>\n",
              "      <td>0.974812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{0: 0.0, 1: 0.0, 2: -0.2, 3: 0.0, 4: -0.4}</td>\n",
              "      <td>{0: 0.9294152428611641, 1: -0.2, 2: 1.85883048...</td>\n",
              "      <td>{0: 0.9946310006906043, 1: 1.9892620013812086,...</td>\n",
              "      <td>0.397340</td>\n",
              "      <td>0.929415</td>\n",
              "      <td>0.994631</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        action state  ...  garbage_quantity_at_10 garbage_quantity_at_15\n",
              "0   Initialize     0  ...                0.000000               0.000000\n",
              "1            4     0  ...                0.003404               0.029880\n",
              "2            4     0  ...                0.038436               0.032542\n",
              "3            1     6  ...                0.049084               0.050034\n",
              "4            3     0  ...                0.082868               0.079886\n",
              "5            3     0  ...                0.116132               0.099835\n",
              "6            1     6  ...                0.148278               0.131868\n",
              "7            3     0  ...                0.150229               0.165785\n",
              "8            1     6  ...                0.169476               0.176049\n",
              "9            3     0  ...                0.171187               0.210649\n",
              "10           2     1  ...                0.187061               0.243643\n",
              "11           0     1  ...                0.205541               0.249853\n",
              "12           0     1  ...                0.236491               0.272505\n",
              "13           2     2  ...                0.272436               0.311865\n",
              "14           3     2  ...                0.287300               0.323227\n",
              "15           4     1  ...                0.294672               0.323571\n",
              "16           0     1  ...                0.323097               0.352002\n",
              "17           2     2  ...                0.332318               0.379297\n",
              "18           0     2  ...                0.360128               0.413131\n",
              "19           1     2  ...                0.381079               0.435541\n",
              "20           2     3  ...                0.408303               0.451716\n",
              "21           0     3  ...                0.425891               0.490626\n",
              "22           1     3  ...                0.426210               0.519857\n",
              "23           1     3  ...                0.460061               0.559243\n",
              "24           4     2  ...                0.471168               0.568596\n",
              "25           3     2  ...                0.494598               0.577183\n",
              "26           4     1  ...                0.532968               0.617115\n",
              "27           4     0  ...                0.554329               0.619317\n",
              "28           4     0  ...                0.586947               0.655698\n",
              "29           4     0  ...                0.601921               0.684064\n",
              "30           2     1  ...                0.630380               0.689188\n",
              "31           2     2  ...                0.642488               0.708654\n",
              "32           0     2  ...                0.643221               0.746916\n",
              "33           4     1  ...                0.681302               0.767520\n",
              "34           3     1  ...                0.702967               0.772536\n",
              "35           1     1  ...                0.727132               0.776863\n",
              "36           4     0  ...                0.752278               0.804561\n",
              "37           4     0  ...                0.764357               0.810002\n",
              "38           0     0  ...                0.787915               0.811573\n",
              "39           4     0  ...                0.789271               0.839251\n",
              "40           1     6  ...                0.790563               0.855563\n",
              "41           4     6  ...                0.811682               0.861640\n",
              "42           3     0  ...                0.826448               0.891954\n",
              "43           0     0  ...                0.856217               0.917502\n",
              "44           4     0  ...                0.859636               0.927191\n",
              "45           0     0  ...                0.875132               0.939335\n",
              "46           0     0  ...                0.892614               0.967982\n",
              "47           3     0  ...                0.929005               0.974812\n",
              "48           0     0  ...                0.929415               0.994631\n",
              "\n",
              "[49 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBqeFbqXuPu"
      },
      "source": [
        "---\r\n",
        "#Building the Brain\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0BmFU-d3h0S"
      },
      "source": [
        "#Build the Brain\r\n",
        "'''\r\n",
        "BRAIN PARAMETERS (TBD):\r\n",
        "\r\n",
        "Input Layer: Should we just have the current state as the input or the current state\r\n",
        "            along with the garbage levels / rewards of the garbage locations as the input\r\n",
        "Output layer: output layer will have 5 nodes corresponding to the Q values\r\n",
        "            of the 5 possible actions possible for each state\r\n",
        "Dense layers: 2 dense layers with 10 nodes each #suggestion\r\n",
        "Compiler:\r\n",
        "    loss:'mse'\r\n",
        "    optimizer: 'Adam'\r\n",
        "    learning_rate= 0.001\r\n",
        "    #suggestions\r\n",
        "'''\r\n",
        "\r\n",
        "# BUILDING THE BRAIN\r\n",
        "\r\n",
        "class Brain(object):\r\n",
        "    \r\n",
        "    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\r\n",
        "    \r\n",
        "    def __init__(self, learning_rate = 0.001, number_of_state_params = 4, number_actions = 5):\r\n",
        "        \r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        \r\n",
        "        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE\r\n",
        "        states = Input(shape = (number_of_state_params,))\r\n",
        "        \r\n",
        "        # BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\r\n",
        "        x = Dense(units = 64, activation = 'sigmoid')(states)\r\n",
        "        x = Dropout(rate = 0.1)(x)\r\n",
        "        \r\n",
        "        # BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\r\n",
        "        y = Dense(units = 32, activation = 'sigmoid')(x)\r\n",
        "        y = Dropout(rate = 0.1)(y)\r\n",
        "        \r\n",
        "        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\r\n",
        "        q_values = Dense(units = number_actions, activation = 'softmax')(y)\r\n",
        "        \r\n",
        "        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\r\n",
        "        self.model = Model(inputs = states, outputs = q_values)\r\n",
        "        \r\n",
        "        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\r\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8nbqVX1X6AR"
      },
      "source": [
        "---\r\n",
        "#Creating a DQN Object (Agent)\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxUcq-GG3jeb"
      },
      "source": [
        "#Build the DQN object = Agent\r\n",
        "'''\r\n",
        "PARAMETERS:\r\n",
        "\r\n",
        "memory: list of length memory_len (we can model this as a queue)\r\n",
        "memory_max_len: length of the memory = 50 (approximately 2 days worth of timesteps)\r\n",
        "discount: 0.9\r\n",
        "'''\r\n",
        "\r\n",
        "class DQN():\r\n",
        "\r\n",
        "    def __init__(self, max_memory = 50, discount = 0.9):\r\n",
        "        self.memory = list()\r\n",
        "        self.max_memory = max_memory\r\n",
        "        self.discount = discount\r\n",
        "\r\n",
        "    '''\r\n",
        "    METHODS:\r\n",
        "\r\n",
        "    get_action:\r\n",
        "        take the current_state as input and predict the action\r\n",
        "        using epsilon delta: random or argmax(predicted Q_values for current_state)\r\n",
        "        returns: action\r\n",
        "    '''\r\n",
        "    def get_action (self, model, current_state, epsilon=0.3)       #####see comment \r\n",
        "        \r\n",
        "        # PLAYING THE NEXT ACTION BY EXPLORATION\r\n",
        "        if np.random.rand() <= epsilon:\r\n",
        "            action = np.random.randint(0, number_actions)\r\n",
        "        \r\n",
        "        # PLAYING THE NEXT ACTION BY INFERENCE\r\n",
        "        else:\r\n",
        "            q_values = model.predict(np.array(current_state))   #current_state.shape = (4,)\r\n",
        "            action = np.argmax(q_values[0])\r\n",
        "        \r\n",
        "        return action \r\n",
        "\r\n",
        "    '''\r\n",
        "    update_memory:\r\n",
        "        get the transition made in the current timestep and append it to memory\r\n",
        "        #transition is defined by [current_state=(state + garbage_info), action, next_state, reward]\r\n",
        "        state = [state, garbage_quantity[3], garbage_quantity[7], garbage_quantity[21]]\r\n",
        "        if len of memory is == memory_max_len: pop(first element)\r\n",
        "    '''\r\n",
        "\r\n",
        "    def update_memory(self, transition):\r\n",
        "        self.memory.append(transition)\r\n",
        "        if len(self.memory) > self.max_memory:\r\n",
        "            del self.memory[0]\r\n",
        "\r\n",
        "    '''\r\n",
        "    get_batch:\r\n",
        "        batch_size: 10\r\n",
        "        input_batch = randomly select min(batch_size, len(memory)) number of states\r\n",
        "                    from the memory\r\n",
        "        target_batch = for each of the states in the input_batch, \r\n",
        "                    corresponding element of the target_batch will be a list containing\r\n",
        "                    the Q values for all the possible actions for that state\r\n",
        "                    predicted by the Brain; here we will subsequently have to update\r\n",
        "                    the Q_value corresponding to the action played (from the memory)\r\n",
        "                    to reward + discount * max(predicted values for next_state)\r\n",
        "    '''\r\n",
        "    def get_batch(self, model, batch_size = 10):\r\n",
        "        len_memory = len(self.memory)\r\n",
        "        num_inputs = len(self.memory[0][0])   #fist element of the memory = transition, first element of transition = state, input = state\r\n",
        "        num_outputs = model.output_shape[-1]\r\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))\r\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\r\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\r\n",
        "            current_state, action, next_state, reward = self.memory[idx][0] \r\n",
        "            inputs[i] = current_state\r\n",
        "            targets[i] = model.predict(current_state)[0]\r\n",
        "            Q_sa = np.max(model.predict(next_state)[0])\r\n",
        "            targets[i, action] = reward + self.discount * Q_sa\r\n",
        "        return inputs, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSKHFWvvYKhz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "#Training\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZVxmdiB3ni8"
      },
      "source": [
        "#Train the Model and save it\r\n",
        "'''\r\n",
        "Create an object of Environment, Brain and DQN Agent\r\n",
        "create a model of brain class\r\n",
        "Number_epochs: number of epochs\r\n",
        "For each epoch:\r\n",
        "    current_state = environment.reset()\r\n",
        "    action = DQN.get_action(current_state)\r\n",
        "    next_state, reward, _, _ = environment.step(action)\r\n",
        "    update the memory: DQN.update_memory(transition=[current_state, action, next_state, reward])\r\n",
        "    get the batches of input and output:\r\n",
        "    batch_input, batch_target = DQN.get_batch()\r\n",
        "    train on the batch and update the loss:\r\n",
        "    loss += model.train_on_batch(batch_input, batch_target)\r\n",
        "    update the total reward\r\n",
        "    keep a track of the timestep\r\n",
        "    each epoch can be decided to run for a certain number of timesteps\r\n",
        "print the results of the training\r\n",
        "save the model for testing/ simulation    \r\n",
        "'''\r\n",
        "\r\n",
        "#Set the parameters:\r\n",
        "epsilon = 0.3   #for epsilon delta exploration\r\n",
        "number_actions = 5\r\n",
        "number_epochs = 10\r\n",
        "max_memory = 50\r\n",
        "discount = 0.9\r\n",
        "batch_size = 10\r\n",
        "\r\n",
        "'''\r\n",
        "Set environment parameters and include arguments in the environment class in case\r\n",
        "parameters of the environment such as:\r\n",
        "    item{garbage_update\r\n",
        "    garbage_threshold\r\n",
        "    living_penalty\r\n",
        "need to be tuned in the environment object.\r\n",
        "'''\r\n",
        "\r\n",
        "#Create the environment as an object of environmemt class\r\n",
        "env = TrashEnv()\r\n",
        "\r\n",
        "#Create the Deep Neural Network as an object of the Brain class\r\n",
        "brain = Brain()\r\n",
        "model = brain.model\r\n",
        "\r\n",
        "#Create the Agent as an object of the DQN class\r\n",
        "dqn = DQN(max_memory, discount)\r\n",
        "\r\n",
        "#Set mode to training:\r\n",
        "env.train = 'Train'\r\n",
        "\r\n",
        "#Start the training\r\n",
        "if env.train == 'Train':\r\n",
        "    for epoch in range(number_epochs):\r\n",
        "        total_reward = 0.0\r\n",
        "        loss = 0.0\r\n",
        "        current_state = env.reset()\r\n",
        "        while timestep <= 500:\r\n",
        "            action = dqn.get_action(model=model, current_state=current_state, epsilon=epsilon)\r\n",
        "            next_state, reward, _, _ = env.step(action)\r\n",
        "            transition=[current_state, action, next_state, reward]\r\n",
        "            dqn.update_memory(transition)\r\n",
        "            batch_input, batch_target = dqn.get_batch(model, batch_size = batch_size)\r\n",
        "            loss += model.train_on_batch(batch_input, batch_target)\r\n",
        "            current_state = next_state\r\n",
        "        #Print the results for the current epoch\r\n",
        "        print(\"\\n\")\r\n",
        "        print(f'Epoch: {epoch}/{number_epochs}')\r\n",
        "        print(f'Total loss over the epoch: {loss}')\r\n",
        "        # EARLY STOPPING CAN BE IMPLEMENTED LATER\r\n",
        "        \r\n",
        "        # SAVING THE MODEL\r\n",
        "        model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsH15ZSYYQ-G"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "#Testing\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lga3zaOt3rK6"
      },
      "source": [
        "#Test the model on a similar environment and publish the results\r\n",
        "'''\r\n",
        "build a simulation environment\r\n",
        "load the saved model\r\n",
        "current_state=environment.reset()\r\n",
        "run simulation for timestep < some value:\r\n",
        "    q_values = model.predict(current_state)\r\n",
        "    action = np.argmax(q_values)\r\n",
        "    next_state, reward, _, _ = environment.step(action)\r\n",
        "    current_state = next_state\r\n",
        "    update reward\r\n",
        "    #maintain a counter for each time the trash in a particular bin 70%:\r\n",
        "        if abs(reward[3] at current timestep - reward[3] at last timestep) \r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiIxyQvNZGtS"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "#Printing and Visualizing the Results\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu5_cz_ZZNB3"
      },
      "source": [
        "#Print the results\r\n",
        "#Report the observations\r\n",
        "#Create a visualization\r\n",
        "#Write a Conclusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-BF4RSdfOdu"
      },
      "source": [
        "#Animation of the results\r\n",
        "\r\n",
        "#convert results in desired format (TBD)\r\n",
        "results = []\r\n",
        "\r\n",
        "#Making the grid\r\n",
        "N = 6\r\n",
        "M = 7\r\n",
        "Roads = np.ones((N, M)) * np.nan    #Empty set\r\n",
        "fig, ax = plt.subplots(1, 1, tight_layout=True)  #fig + axes\r\n",
        "Roads_cmap = matplotlib.colors.ListedColormap(['Grey']) #grey for borders\r\n",
        "for x in range(N + 1):      #draw grid\r\n",
        "    for y in range(M+1):\r\n",
        "        ax.axhline(x, lw=2, color='k', zorder=5)\r\n",
        "        ax.axvline(x, lw=2, color='k', zorder=5)\r\n",
        "ax.imshow(Roads, interpolation='none', cmap=Roads_cmap, extent=[0, N, 0, M], zorder=0)   #roads\r\n",
        "ax.axis('off')   #remove axis\r\n",
        "\r\n",
        "#Highlighting pre-defined cells (Roads & pickups)\r\n",
        "\r\n",
        "#Creating animation\r\n",
        "from matplotlib.animation import FuncAnimation\r\n",
        "\r\n",
        "fig, ax = plt.subplots()\r\n",
        "xdata, ydata = [], []\r\n",
        "ln, = plt.plot([], [], 'ro')\r\n",
        "\r\n",
        "def init():\r\n",
        "    ax.set_xlim(0, 2*np.pi)\r\n",
        "    ax.set_ylim(-1, 1)\r\n",
        "    return ln,\r\n",
        "\r\n",
        "def update(frame):\r\n",
        "    xdata.append(frame)\r\n",
        "    ydata.append(np.sin(frame))\r\n",
        "    ln.set_data(xdata, ydata)\r\n",
        "    return ln,\r\n",
        "\r\n",
        "ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 128),\r\n",
        "                    init_func=init, blit=True)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}