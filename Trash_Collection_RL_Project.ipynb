{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trash_Collection_RL_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sattwiksuman/AI_for_Trash_Collection/blob/discrete_environment/Trash_Collection_RL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV8MUqYr-tHQ"
      },
      "source": [
        "#<center>Efficient Trash Collection using AI</center>\n",
        "---\n",
        "####<center>Reinforced Learning Project - Economic Modelling of Energy and Climate Systems</center>\n",
        "####<center>Data Analytics and Decision Science</center>\n",
        "####<center>RWTH Aachen Business School</center>\n",
        "Submitted by:<br>\n",
        "Priyanka Kundagol<br>\n",
        "Sattwik Suman Das<br>\n",
        "Ved Nerlikar\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3urOwETcGek"
      },
      "source": [
        "---\n",
        "#Introduction\n",
        "\n",
        "---\n",
        "\n",
        "Combinatorial Optimization problems such as vehicle routing problem or travelling salesman problems have been traditionally solved by using Operations Research concepts and Heuristic approaches. In the recent years however, Reinforcement Learning techniques based on Markov Decision Process and Q-learning have been implemented to explore training Deep Neural Networks to learn the optimal policy for an agent’s actions based on an appropriately implemented environment and suitably designed reward function [2][3].\n",
        "\n",
        "In our quest for a topic that used Reinforcement Learning methodologies to model climate effects, we stumbled upon a paper “Automated Trash Collection using Markov Decision Processes” by Robert J. Moss, Stanford [1]. It gave us an insight into how a grid-based environment can be simulated to model the roads of a city and an agent can be trained to collect garbage from specific locations. We implemented this as part of our project. The details of the implementation can be seen in the following blocks of the notebook.\n",
        "\n",
        "Trash is usually collected by the municipal bodies by trucks that run on pre-defined routes at pre-defined intervals of time. The paper [1] proposes a smart system where the truck automatically senses the level of trash at each garbage location and determines the best path it can take to collect the trash. This is expected to optimize the distance covered by these trucks thus helping in fuel savings and more efficient scheduling.\n",
        "\n",
        "Further applications of this principle might also be in automated trash collection from garbage bins at hospital corridors. We also see similar applications in warehouses [3] or production lines where robots can be used to automatically replenish items at the workstations. All these applications are motivated by enhancing the efficiency of operations which can manifest into several benefits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPGv-8QnZ3Wg"
      },
      "source": [
        "###The Environment\n",
        "The Diagram below shows the environment in which the Agent would operate.\n",
        "The squares in gray are the road.\n",
        "The Agent is expected to collect trash from the location shown in Black boxes.\n",
        "The trash is actually collected from the adjacent squares marked by a dark border on the road, i.e., squares 3, 17 and 21."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDkpXo769ZQL"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZ8Ya4s_83u"
      },
      "source": [
        "###States of the Environment:\n",
        "The gray squares above depict the states in which the agent can reside. <br>\n",
        "So there are 26 states in total numbered 0 to 25. <br>\n",
        "The movement of the agent from one state to another is defined by defining the rewards of the adjacent states.\n",
        "\n",
        "###Actions\n",
        "At each state the agent can 5 actions as follows:\n",
        "0. Do nothing\n",
        "1. Move North\n",
        "2. Move East\n",
        "3. Move South\n",
        "4. Move West \n",
        "\n",
        "###Action_Space: control the motion\n",
        "For each state, the next state corresponding to the consequence of the 5 actions will be pre-defined as part of the environment definition. \n",
        "\n",
        "###Rewards\n",
        "The reward of the trash locations 03, 17 and 21 are defined to reflect the amount of trash in the following manner:\n",
        "1. if trash >= threshold:\n",
        "        reward= percentage of trash bin full\n",
        "2. else:\n",
        "        reward = 0\n",
        "<br>\n",
        "The threshold can be set to 70% .\n",
        "\n",
        "###Reward for road states in form of Living Penalty\n",
        "A Living Penalty is defined to each of the states on the road because we want the agent to move by taking the shortest time, which would ultimately be the most efficient way to travel. <br>\n",
        "Living Penalty is not defined to the start location '0' and the trash locations. <br>\n",
        "The agent is expected to reside at the starting point till it needs to make it's first trip and then reside at the location from which it has collected the trash till it needs to move to the next location to collect from the next location.<br>\n",
        "Living penalty can be set to -0.2\n",
        "\n",
        "###Carrying capacity of the Agent\n",
        "For the initial model, the agent is assumed to have infinite capacity. <br>\n",
        "This effectively means it never has to return to the depot to empty itself.\n",
        "\n",
        "###Timestep of the simulation\n",
        "The timestep can be assumed to be one hour.<br>\n",
        "In each timestep:\n",
        "1. the vehicle is assumed to have one state transition.\n",
        "2. the garbage is updated to new quatities\n",
        "\n",
        "###Implementation history:\n",
        "The environment implementation as described above remained evolved from a first model where the there were just individual square blocks marking the states of the system. In order to reduce the state space for more efficient training, we merged adjacent blocks which were not at a junction or were not garbage locations. The living penalty for these locations was then set according to their size.\n",
        "\n",
        "A simple neural network was first employed with an input layer, two hidden layers with attached dropout layers and one output layer as the brain of the Deep Q Learning algorithm. The model can be seen after the training block below. We further introduced weight initialization using a Xavier initialization approach to get better results.\n",
        "\n",
        "A lot of iterations were made trying out different reward strategies:\n",
        "1.\tUpon trying the first reward function of giving living penalty = -0.2 to all states depending on their size, 0 reward to location 0 and reward = garbage quantity if garbage quantity > 70% for garbage locations, it was seen that the agent preferred to just stay at the origin and accumulate zero reward rather than moving around and accumulating negative rewards.\n",
        "2.\tThen a next attempt was made where the reward function for location 0 was changed to base value of living penalty in case the garbage quantity at any garbage location went above 70%. However, that also didn't improve the result and the agent just wandered around location 0 and didn't collect the garbage at all.\n",
        "3.\tA fundamental error in implementation was detected and the reward function was subsequently scaled by dividing the reward by 1000. Errors in update of reward space in step function of the environment were also corrected and further simulations were done. The Agent was now seen to be moving to the nearest garbage location and settling there rather than collecting the garbage at all locations. It just moves out and enters the garbage location 2 when garbage builds up in that location.\n",
        "4.\tThe next strategy was to increase the base value of living penalty when the garbage value in a garbage location went beyond 0.95 with the aim to not allow the garbage locations to fill beyond 100%. However, this didn’t result in any improvement of the result. Trials were done with different values of base value and increased base value of living penalty.\n",
        "5.\tFurther reading suggested that the number of timesteps on which to train the model must be very large (of the order of million as opposed to roughly 70000 step we had trained at maximum). We are using Google Colab to build and train our model the free version of which has a maximum run time of 12 hours. We tried to save the models and then export pre saved models to continue training. However even with this and the limited time we could afford to devote to training a particular strategy, we were not able to see significant improvement in performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsEbpRcKmvB8"
      },
      "source": [
        "---\n",
        "#Importing the packages\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaMxZ7ET89kS",
        "outputId": "99d31230-6b43-47bd-f487-d94c1aa3a3d6"
      },
      "source": [
        "#Mount the storage space on google drive to save the models and load later\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THtm1DnRBmP8"
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "from random import random\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVSBazS5m3Ej"
      },
      "source": [
        "---\n",
        "#Building the Environment\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MRG3cBD3WPT"
      },
      "source": [
        "#Build the environment\n",
        "\n",
        "class TrashEnv(gym.Env):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TrashEnv, self).__init__()\n",
        "        # Define action and observation space\n",
        "        # Gym environment is not used. Rather a grid world is created from scratch.\n",
        "        n_actions = 5\n",
        "        #self.action_space = spaces.Discrete(n_actions)\n",
        "\n",
        "        self.total_states = 17\n",
        "        self.observation_space = spaces.Discrete(self.total_states)\n",
        "\n",
        "        #Define the action space:\n",
        "        self.action_space = {}\n",
        "        self.action_space[0]={0:0, 1:6, 2:1, 3:0, 4:0}\n",
        "        self.action_space[1]={0:1, 1:1, 2:2, 3:1, 4:0}\n",
        "        self.action_space[2]={0:2, 1:2, 2:3, 3:2, 4:1}\n",
        "        self.action_space[3]={0:3, 1:3, 2:4, 3:3, 4:2}\n",
        "        self.action_space[4]={0:4, 1:5, 2:4, 3:4, 4:3}\n",
        "        self.action_space[5]={0:5, 1:9, 2:5, 3:4, 4:5}\n",
        "        self.action_space[6]={0:6, 1:7, 2:6, 3:0, 4:6}\n",
        "        self.action_space[7]={0:7, 1:11, 2:8, 3:6, 4:7}\n",
        "        self.action_space[8]={0:8, 1:8, 2:9, 3:8, 4:7}\n",
        "        self.action_space[9]={0:9, 1:10, 2:9, 3:5, 4:8}\n",
        "        self.action_space[10]={0:10, 1:12, 2:10, 3:9, 4:10}\n",
        "        self.action_space[11]={0:11, 1:16, 2:11, 3:7, 4:11}\n",
        "        self.action_space[12]={0:12, 1:13, 2:12, 3:10, 4:12}\n",
        "        self.action_space[13]={0:13, 1:13, 2:13, 3:12, 4:14}\n",
        "        self.action_space[14]={0:14, 1:14, 2:13, 3:14, 4:15}\n",
        "        self.action_space[15]={0:15, 1:15, 2:14, 3:15, 4:16}\n",
        "        self.action_space[16]={0:16, 1:16, 2:15, 3:11, 4:16}\n",
        "        \n",
        "        #Define the Garbage locations and corresponding initial garbage quantity values = 0:\n",
        "        self.garbage_quantity = {}\n",
        "        self.garbage_locations = [2, 10, 15]\n",
        "        for l in self.garbage_locations:\n",
        "            self.garbage_quantity[l]=0.0  \n",
        "\n",
        "        #Define maximum quatity of garbage that would be updated in one timestep:\n",
        "        self.max_garbage_update = 0.04   \n",
        "\n",
        "        #Define threshold after which garbage updation reflects in reward:\n",
        "        self.garbage_threshold = 0.7\n",
        "        \n",
        "        # Set the living penalty\n",
        "        self.base_value = -1\n",
        "        self.high_value = -10\n",
        "        self.living_penalty_base_value = self.base_value\n",
        "        \n",
        "        # living penalty is assigned according to the size of the state: refer to diagram attached above\n",
        "        self.living_penalty = self.update_living_penalty()\n",
        "        \n",
        "        #Define the Rewards:\n",
        "        #reward_space is defined as a dictionary of dictionaries {current_state:{action:reward}}\n",
        "        self.reward_space = {}\n",
        "        for i in range(len(self.action_space)):\n",
        "            self.reward_space[i] = self.update_reward(i)\n",
        "\n",
        "        # Create state attribute, initialize it in reset method\n",
        "        self.state = None\n",
        "\n",
        "        #Create a variable to measure the distance covered\n",
        "        #This will be compared against the distance covered in the naive approach\n",
        "        self.distance_covered = 0\n",
        "\n",
        "        #Create a variable to store the number of timesteps\n",
        "        #This can be used to define an epoch\n",
        "        self.current_time = 0\n",
        "\n",
        "        #Create a variable to set the status of training to Train or Test or None\n",
        "        #This allows us not to run the training everytime we run the whole notebook\n",
        "        self.train = None\n",
        "        \n",
        "        \n",
        "    def step(self, action):\n",
        "        \"\"\"State transition of the model.\n",
        "        Implements the model of the environment.\n",
        "        Args:\n",
        "            action (int): Action the agent took.\n",
        "        Returns:\n",
        "            next_state (int): The next state the environment emits.\n",
        "            reward (float): The reward the environment emits.\n",
        "            done (bool): Currently always set to 0 as we are modelling a continuous process\n",
        "            garbage_info (dict): Info about the rewards at the garbage locations {loc: reward}\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate the next state\n",
        "        next_state = self.action_space[self.state[0]][action]\n",
        "\n",
        "        # Calculate the reward\n",
        "        ##Reward can not be set after update of the reward_space following the action\n",
        "        ##because the reward_space will update the garbage location to zero in case the agent arrives there giving the agent 0 reward.\n",
        "        reward = self.reward_space[self.state[0]][action]\n",
        "        reward = 1e-3 * reward  #scaling of reward\n",
        "\n",
        "        #If reward of any garbage location is non zero, then living penalty at location 0 is 2 * living penalty base value\n",
        "        #If the garbage quantity at any garbage location reaches 0.95, the living penalty base value increases\n",
        "        #Otherwise the agent just learns to stay at the location 0 as it maximizes reward. \n",
        "        if all(x==0 for x in [self.reward_space[l][0] for l in self.garbage_locations]):\n",
        "            self.living_penalty[0]=0\n",
        "        else:\n",
        "            if True in [item>0.95 for item in [self.reward_space[l][0] for l in self.garbage_locations]]:\n",
        "                self.living_penalty_base_value = self.high_value\n",
        "                self.living_penalty = self.update_living_penalty()\n",
        "            else:\n",
        "                self.living_penalty_base_value = self.base_value\n",
        "                self.living_penalty = self.update_living_penalty()\n",
        "            self.living_penalty[0]=2*self.living_penalty_base_value\n",
        "        self.reward_space[0]=self.update_reward(0)\n",
        "\n",
        "        #Update the garbage_quantity and reward_space:\n",
        "        if next_state not in self.garbage_locations:    #in free area or stepping out of garbage location\n",
        "            for l in self.garbage_locations:\n",
        "                self.garbage_quantity[l]+=random()*self.max_garbage_update\n",
        "                for i in range(len(self.action_space)):\n",
        "                    self.reward_space[i] = self.update_reward(i)\n",
        "        else:\n",
        "            if self.state[0] not in self.garbage_locations:\n",
        "                for l in self.garbage_locations:\n",
        "                    if l == next_state:         #stepping into garbage location\n",
        "                        self.garbage_quantity[l]=0.0\n",
        "                        for i in range(len(self.action_space)):\n",
        "                            self.reward_space[i] = self.update_reward(i)\n",
        "                    else:       #this else statement should never be true\n",
        "                        self.garbage_quantity[l]+=random()*self.max_garbage_update\n",
        "                        self.reward_space[l]=self.update_reward(l)\n",
        "            else:       #waiting at a garbage location\n",
        "                for l in self.garbage_locations:\n",
        "                    self.garbage_quantity[l]+=random()*self.max_garbage_update\n",
        "                    for i in range(len(self.action_space)):\n",
        "                        self.reward_space[i] = self.update_reward(i)\n",
        "\n",
        "        #The episode will be continuous, so there will be no 'Game Over'/ 'Done'\n",
        "        done = 0\n",
        "\n",
        "        self.garbage_info = {self.garbage_locations[i]: self.reward_space[l] for i, l in enumerate(self.garbage_locations)}\n",
        "\n",
        "        #Update the distance covered\n",
        "        if self.state[0] != next_state:\n",
        "            self.distance_covered += 1 \n",
        "\n",
        "        #Update the current time\n",
        "        self.current_time += 1\n",
        "\n",
        "        #Update the state to the next state:\n",
        "        self.state = [next_state]\n",
        "        for l in self.garbage_locations:\n",
        "            self.state.append(self.garbage_quantity[l])\n",
        "        return self.state, reward, done, self.garbage_info\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment.\n",
        "        Initializes the state.\n",
        "        Returns:\n",
        "            state (int): [state, garbage_quantity at garbage locations]\n",
        "            garbage_info (dict): Info about the rewards at the garbage locations {loc: reward}\n",
        "        \"\"\"\n",
        "        self.state = [0]\n",
        "        for l in self.garbage_locations:\n",
        "            self.garbage_quantity[l]=0.0\n",
        "            self.state.append(self.garbage_quantity[l])\n",
        "            self.reward_space[l]=self.update_reward(l)\n",
        "        self.distance_covered = 0\n",
        "        self.current_time = 0\n",
        "        self.garbage_info = {self.garbage_locations[i]: self.reward_space[l] for i, l in enumerate(self.garbage_locations)}\n",
        "        return self.state\n",
        "\n",
        "    #Update living penalty at all locations:\n",
        "    def update_living_penalty(self):\n",
        "        self.living_penalty = {0:0,              #reward for initial location must be zero as we allow the agent to reside at 0 before it has to start\n",
        "                            1:2*self.living_penalty_base_value,\n",
        "                            2:0,              #reward for garbage location would be assigned later. it is just initialized with 0.\n",
        "                            3:self.living_penalty_base_value,\n",
        "                            4:self.living_penalty_base_value,\n",
        "                            5:2*self.living_penalty_base_value,\n",
        "                            6:2*self.living_penalty_base_value,\n",
        "                            7:self.living_penalty_base_value, \n",
        "                            8:4*self.living_penalty_base_value,\n",
        "                            9:self.living_penalty_base_value,\n",
        "                            10:0,\n",
        "                            11:2*self.living_penalty_base_value,\n",
        "                            12:self.living_penalty_base_value,\n",
        "                            13:self.living_penalty_base_value,\n",
        "                            14:3*self.living_penalty_base_value,\n",
        "                            15:0,\n",
        "                            16:self.living_penalty_base_value\n",
        "                            }\n",
        "        return self.living_penalty\n",
        "\n",
        "    #Define update reward function:\n",
        "    def update_reward(self, location):\n",
        "        i= location\n",
        "        dummy_dict={}\n",
        "        for act, j in zip(self.action_space[i].keys(), self.action_space[i].values()):\n",
        "            dummy_dict[act]=self.living_penalty[j]\n",
        "            if j in self.garbage_locations:\n",
        "                dummy_dict[act]=self.update_garbage_reward(j)\n",
        "            if act!=0 and j==i:     #larger penalty for infeasible actions\n",
        "                dummy_dict[act]=2*dummy_dict[act]\n",
        "                if i in self.garbage_locations:\n",
        "                    dummy_dict[act]=self.living_penalty_base_value\n",
        "        return dummy_dict\n",
        "\n",
        "    def update_garbage_reward(self, garbage_location):\n",
        "        \"\"\"Updates the reward space at a garbage location based on the quantity of garbage at the location\n",
        "        Returns:\n",
        "            reward_space[garbage_location]\n",
        "        \"\"\"\n",
        "        i=garbage_location\n",
        "        if i in self.garbage_locations:\n",
        "                if self.garbage_quantity[i] > 0.7:\n",
        "                    dummy_reward=self.garbage_quantity[i]*100 \n",
        "                else:\n",
        "                    dummy_reward=0.0\n",
        "        return(dummy_reward)        #self.reward_space[i]\n",
        "\n",
        "    # We will not implement render and close function\n",
        "    def render(self, mode='human'):\n",
        "        pass\n",
        "    def close (self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8jk3YmRmaiE"
      },
      "source": [
        "---\n",
        "#Testing the environment\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oArN8erumbM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4aafcb-0f5c-4ca2-a23a-32f8da1dd85c"
      },
      "source": [
        "# Create the environment\n",
        "test_env = TrashEnv()\n",
        "\n",
        "# Reset the environment to get initial state\n",
        "state = test_env.reset()\n",
        "state = state[0]\n",
        "print(test_env.garbage_info)\n",
        "print(test_env.reward_space[2])\n",
        "\n",
        "# Create a list to store all states during the simulation\n",
        "columns=['action', 'state', 'reward']\n",
        "for key in test_env.garbage_info.keys():\n",
        "    columns.append(f'reward_at_garbage_{key}')\n",
        "for key in test_env.garbage_info.keys():\n",
        "    columns.append(f'garbage_quantity_at_{key}')\n",
        "\n",
        "row = {'action': 'Initialize', 'state':state, 'reward': 0.0}\n",
        "for key, val in zip(columns[3:6], test_env.garbage_info.values()):\n",
        "    row[key]=val\n",
        "for key, val in zip(columns[6:], test_env.garbage_quantity.values()):\n",
        "    row[key]=val\n",
        "print(row)\n",
        "\n",
        "#Pandas DataFrame test_log stores the state, action, reward and garbage parameters at each timestep. \n",
        "test_log = pd.DataFrame(columns=columns)\n",
        "test_log = test_log.append(row, ignore_index=True)  #the first row is the start position\n",
        "\n",
        "# Loop over each time step in the episode\n",
        "done = False\n",
        "for _ in range(100):\n",
        "    action = randint(0,4)\n",
        "    state, reward, _, garbage_info = test_env.step(action)\n",
        "    state=state[0]\n",
        "    #print('state', state)  #for debugging\n",
        "    row = {'action':action, 'state':state, 'reward': reward}\n",
        "    for key, val in zip(columns[3:], garbage_info.values()):\n",
        "        row[key]=val\n",
        "    for key, val in zip(columns[6:], test_env.garbage_quantity.values()):\n",
        "        row[key]=val\n",
        "    test_log = test_log.append(row, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{2: {0: 0.0, 1: -1, 2: -1, 3: -1, 4: -2}, 10: {0: 0.0, 1: -1, 2: -1, 3: -1, 4: -1}, 15: {0: 0.0, 1: -1, 2: -3, 3: -1, 4: -1}}\n",
            "{0: 0.0, 1: -1, 2: -1, 3: -1, 4: -2}\n",
            "{'action': 'Initialize', 'state': 0, 'reward': 0.0, 'reward_at_garbage_2': {0: 0.0, 1: -1, 2: -1, 3: -1, 4: -2}, 'reward_at_garbage_10': {0: 0.0, 1: -1, 2: -1, 3: -1, 4: -1}, 'reward_at_garbage_15': {0: 0.0, 1: -1, 2: -3, 3: -1, 4: -1}, 'garbage_quantity_at_2': 0.0, 'garbage_quantity_at_10': 0.0, 'garbage_quantity_at_15': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2427
        },
        "id": "oYh3L01UCzOB",
        "outputId": "a97b2585-956f-484e-f166-39f4eef2e0a8"
      },
      "source": [
        " test_log.loc[50:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>action</th>\n",
              "      <th>state</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_at_garbage_2</th>\n",
              "      <th>reward_at_garbage_10</th>\n",
              "      <th>reward_at_garbage_15</th>\n",
              "      <th>garbage_quantity_at_2</th>\n",
              "      <th>garbage_quantity_at_10</th>\n",
              "      <th>garbage_quantity_at_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>{0: 85.2906366846082, 1: -10, 2: -10, 3: -10, ...</td>\n",
              "      <td>{0: 113.6349912426506, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.852906</td>\n",
              "      <td>1.136350</td>\n",
              "      <td>0.097019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 88.02770193457565, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 115.06173807157829, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.880277</td>\n",
              "      <td>1.150617</td>\n",
              "      <td>0.122919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 89.85457120287138, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 118.21434383057965, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.898546</td>\n",
              "      <td>1.182143</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 93.28769844268045, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 121.02152857929305, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.932877</td>\n",
              "      <td>1.210215</td>\n",
              "      <td>0.004039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 95.05761651758111, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 124.28186988551798, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.950576</td>\n",
              "      <td>1.242819</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 95.76219338883533, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 125.20492394302853, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.957622</td>\n",
              "      <td>1.252049</td>\n",
              "      <td>0.008529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 96.6662476090209, 1: -10, 2: -10, 3: -10, ...</td>\n",
              "      <td>{0: 128.70815108109585, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.966662</td>\n",
              "      <td>1.287082</td>\n",
              "      <td>0.023869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>{0: 98.04093554267507, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 132.45924420143083, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.980409</td>\n",
              "      <td>1.324592</td>\n",
              "      <td>0.041341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 99.22407148408828, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 133.6394281209433, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.992241</td>\n",
              "      <td>1.336394</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 99.31718326274532, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 136.0610365117154, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.993172</td>\n",
              "      <td>1.360610</td>\n",
              "      <td>0.019751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 99.60838246429896, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 136.73191448420795, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>0.996084</td>\n",
              "      <td>1.367319</td>\n",
              "      <td>0.031894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 101.73629032032112, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 138.38596763242813, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.017363</td>\n",
              "      <td>1.383860</td>\n",
              "      <td>0.048023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 103.60802898989225, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 141.8800772141285, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.036080</td>\n",
              "      <td>1.418801</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 104.75944540872307, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 142.40542289151193, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.047594</td>\n",
              "      <td>1.424054</td>\n",
              "      <td>0.031954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 105.01037839935752, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 145.54913072759757, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.050104</td>\n",
              "      <td>1.455491</td>\n",
              "      <td>0.064380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 108.40846118690305, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 148.18285313031356, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.084085</td>\n",
              "      <td>1.481829</td>\n",
              "      <td>0.092416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 110.32249547103146, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 150.40988720709626, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.103225</td>\n",
              "      <td>1.504099</td>\n",
              "      <td>0.101152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 110.78176829178177, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 151.60348282554318, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.107818</td>\n",
              "      <td>1.516035</td>\n",
              "      <td>0.105230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 113.30955149834428, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 152.50753635599295, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.133096</td>\n",
              "      <td>1.525075</td>\n",
              "      <td>0.132764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>{0: 114.11438859398513, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 156.3005835961762, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.141144</td>\n",
              "      <td>1.563006</td>\n",
              "      <td>0.152770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 114.47342851397539, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 158.6272485436072, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.144734</td>\n",
              "      <td>1.586272</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 115.64968624278993, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 161.69028365842664, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.156497</td>\n",
              "      <td>1.616903</td>\n",
              "      <td>0.009808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 118.42591771550552, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 162.8045818245406, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.184259</td>\n",
              "      <td>1.628046</td>\n",
              "      <td>0.023315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 119.60513519859572, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 164.63585596396078, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.196051</td>\n",
              "      <td>1.646359</td>\n",
              "      <td>0.036936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 122.27983939864713, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 168.61687122514257, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.222798</td>\n",
              "      <td>1.686169</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 124.19499158923767, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 168.93092332808104, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.241950</td>\n",
              "      <td>1.689309</td>\n",
              "      <td>0.023633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 126.7868801090843, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 170.38955120087263, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.267869</td>\n",
              "      <td>1.703896</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 128.6623905678982, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 172.0091585666188, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.286624</td>\n",
              "      <td>1.720092</td>\n",
              "      <td>0.031844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>{0: 129.87579794073, 1: -10, 2: -10, 3: -10, 4...</td>\n",
              "      <td>{0: 173.63991185450706, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.298758</td>\n",
              "      <td>1.736399</td>\n",
              "      <td>0.032484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 132.00209389154767, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 177.00938431520413, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.320021</td>\n",
              "      <td>1.770094</td>\n",
              "      <td>0.036817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>{0: 133.48339575935697, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 177.3002542567619, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.334834</td>\n",
              "      <td>1.773003</td>\n",
              "      <td>0.072919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 135.8228911762198, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 179.4431091017425, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.358229</td>\n",
              "      <td>1.794431</td>\n",
              "      <td>0.099121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>{0: 136.87503067451973, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 180.19001081149747, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.368750</td>\n",
              "      <td>1.801900</td>\n",
              "      <td>0.138488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 138.80126897705182, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 180.40439207453747, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.388013</td>\n",
              "      <td>1.804044</td>\n",
              "      <td>0.168104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>{0: 141.12278175678105, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 182.2310106975552, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.411228</td>\n",
              "      <td>1.822310</td>\n",
              "      <td>0.200762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 143.70096520785023, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 182.8682215287299, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.437010</td>\n",
              "      <td>1.828682</td>\n",
              "      <td>0.221304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 147.46507658159382, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 184.15403500599115, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.474651</td>\n",
              "      <td>1.841540</td>\n",
              "      <td>0.258236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>{0: 149.07961367216268, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 186.23227837575072, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.490796</td>\n",
              "      <td>1.862323</td>\n",
              "      <td>0.271080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>{0: 150.36523511923744, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 188.57107910812894, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.503652</td>\n",
              "      <td>1.885711</td>\n",
              "      <td>0.292234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 152.87189399011635, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 191.96824874686033, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.528719</td>\n",
              "      <td>1.919682</td>\n",
              "      <td>0.293230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 155.56265048821132, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 194.69098873366357, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.555627</td>\n",
              "      <td>1.946910</td>\n",
              "      <td>0.307941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 157.84486218007459, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 196.5968235508166, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.578449</td>\n",
              "      <td>1.965968</td>\n",
              "      <td>0.342170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 159.9398843867387, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 196.97554397716934, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.599399</td>\n",
              "      <td>1.969755</td>\n",
              "      <td>0.359151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 162.32508485346779, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 197.28472619075137, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.623251</td>\n",
              "      <td>1.972847</td>\n",
              "      <td>0.388024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 163.05091110642218, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 201.12021247975167, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.630509</td>\n",
              "      <td>2.011202</td>\n",
              "      <td>0.427379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 163.94147643266317, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 202.28072022122063, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.639415</td>\n",
              "      <td>2.022807</td>\n",
              "      <td>0.452864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 164.88931022693905, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 205.88577672020403, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.648893</td>\n",
              "      <td>2.058858</td>\n",
              "      <td>0.482249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 165.0906072111778, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 208.28085977118383, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.650906</td>\n",
              "      <td>2.082809</td>\n",
              "      <td>0.521306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 169.03841559628586, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 208.61534436652263, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.690384</td>\n",
              "      <td>2.086153</td>\n",
              "      <td>0.560145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>{0: 171.28099534577694, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 211.53614363464536, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.712810</td>\n",
              "      <td>2.115361</td>\n",
              "      <td>0.573963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>{0: 172.6631656475167, 1: -10, 2: -10, 3: -10,...</td>\n",
              "      <td>{0: 214.08713996961927, 1: -10, 2: -10, 3: -10...</td>\n",
              "      <td>{0: 0.0, 1: -10, 2: -30, 3: -10, 4: -10}</td>\n",
              "      <td>1.726632</td>\n",
              "      <td>2.140871</td>\n",
              "      <td>0.577972</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    action state  ...  garbage_quantity_at_10 garbage_quantity_at_15\n",
              "50       4    16  ...                1.136350               0.097019\n",
              "51       0    16  ...                1.150617               0.122919\n",
              "52       2    15  ...                1.182143               0.000000\n",
              "53       2    14  ...                1.210215               0.004039\n",
              "54       4    15  ...                1.242819               0.000000\n",
              "55       3    15  ...                1.252049               0.008529\n",
              "56       2    14  ...                1.287082               0.023869\n",
              "57       3    14  ...                1.324592               0.041341\n",
              "58       4    15  ...                1.336394               0.000000\n",
              "59       2    14  ...                1.360610               0.019751\n",
              "60       0    14  ...                1.367319               0.031894\n",
              "61       0    14  ...                1.383860               0.048023\n",
              "62       4    15  ...                1.418801               0.000000\n",
              "63       3    15  ...                1.424054               0.031954\n",
              "64       1    15  ...                1.455491               0.064380\n",
              "65       1    15  ...                1.481829               0.092416\n",
              "66       0    15  ...                1.504099               0.101152\n",
              "67       0    15  ...                1.516035               0.105230\n",
              "68       4    16  ...                1.525075               0.132764\n",
              "69       4    16  ...                1.563006               0.152770\n",
              "70       2    15  ...                1.586272               0.000000\n",
              "71       0    15  ...                1.616903               0.009808\n",
              "72       1    15  ...                1.628046               0.023315\n",
              "73       4    16  ...                1.646359               0.036936\n",
              "74       2    15  ...                1.686169               0.000000\n",
              "75       2    14  ...                1.689309               0.023633\n",
              "76       4    15  ...                1.703896               0.000000\n",
              "77       3    15  ...                1.720092               0.031844\n",
              "78       0    15  ...                1.736399               0.032484\n",
              "79       2    14  ...                1.770094               0.036817\n",
              "80       1    14  ...                1.773003               0.072919\n",
              "81       2    13  ...                1.794431               0.099121\n",
              "82       2    13  ...                1.801900               0.138488\n",
              "83       3    12  ...                1.804044               0.168104\n",
              "84       4    12  ...                1.822310               0.200762\n",
              "85       0    12  ...                1.828682               0.221304\n",
              "86       1    13  ...                1.841540               0.258236\n",
              "87       2    13  ...                1.862323               0.271080\n",
              "88       2    13  ...                1.885711               0.292234\n",
              "89       0    13  ...                1.919682               0.293230\n",
              "90       3    12  ...                1.946910               0.307941\n",
              "91       0    12  ...                1.965968               0.342170\n",
              "92       1    13  ...                1.969755               0.359151\n",
              "93       4    14  ...                1.972847               0.388024\n",
              "94       0    14  ...                2.011202               0.427379\n",
              "95       2    13  ...                2.022807               0.452864\n",
              "96       3    12  ...                2.058858               0.482249\n",
              "97       1    13  ...                2.082809               0.521306\n",
              "98       0    13  ...                2.086153               0.560145\n",
              "99       0    13  ...                2.115361               0.573963\n",
              "100      4    14  ...                2.140871               0.577972\n",
              "\n",
              "[51 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBqeFbqXuPu"
      },
      "source": [
        "---\n",
        "#Building the Brain\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0BmFU-d3h0S"
      },
      "source": [
        "#Build the Brain\n",
        "'''\n",
        "BRAIN PARAMETERS (TBD):\n",
        "\n",
        "Input Layer: Should we just have the current state as the input or the current state\n",
        "            along with the garbage levels / rewards of the garbage locations as the input\n",
        "Output layer: output layer will have 5 nodes corresponding to the Q values\n",
        "            of the 5 possible actions possible for each state\n",
        "Dense layers: 2 dense layers with 10 nodes each #suggestion\n",
        "Compiler:\n",
        "    loss:'mse'\n",
        "    optimizer: 'Adam'\n",
        "    learning_rate= 0.001\n",
        "    #suggestions\n",
        "'''\n",
        "\n",
        "# BUILDING THE BRAIN\n",
        "\n",
        "class Brain(object):\n",
        "    \n",
        "    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\n",
        "    \n",
        "    def __init__(self, learning_rate = 0.001, number_of_state_params = 4, number_actions = 5):\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE\n",
        "        states = Input(shape = (number_of_state_params,))\n",
        "\n",
        "        #BUILDING THE INITIALIZER FOR XAVIER WEIGHT INITIALIZATION:\n",
        "        #weights are initialized to sqrt(1/fan_in+fan_out)\n",
        "        initializer = tf.keras.initializers.GlorotUniform()\n",
        "        \n",
        "        # BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n",
        "        x = Dense(units = 64, activation = 'sigmoid', kernel_initializer=initializer)(states)\n",
        "        x = Dropout(rate = 0.1)(x)\n",
        "        \n",
        "        # BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n",
        "        y = Dense(units = 32, activation = 'sigmoid', kernel_initializer=initializer)(x)\n",
        "        y = Dropout(rate = 0.1)(y)\n",
        "        \n",
        "        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\n",
        "        output_initializer = tf.keras.initializers.Zeros()\n",
        "        q_values = Dense(units = number_actions, activation = 'softmax', kernel_initializer = output_initializer)(y)\n",
        "        \n",
        "        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\n",
        "        self.model = Model(inputs = states, outputs = q_values)\n",
        "        \n",
        "        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8nbqVX1X6AR"
      },
      "source": [
        "---\n",
        "#Creating a DQN Object (Agent)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxUcq-GG3jeb"
      },
      "source": [
        "#Build the DQN object = Agent\n",
        "'''\n",
        "PARAMETERS:\n",
        "\n",
        "memory: list of length memory_len (we can model this as a queue)\n",
        "memory_max_len: length of the memory = 50 (approximately 2 days worth of timesteps)\n",
        "discount: 0.9\n",
        "'''\n",
        "\n",
        "class DQN():\n",
        "\n",
        "    def __init__(self, max_memory = 50, discount = 0.9):\n",
        "        self.memory = list()\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "\n",
        "    '''\n",
        "    METHODS:\n",
        "\n",
        "    get_action:\n",
        "        take the current_state as input and predict the action\n",
        "        using epsilon delta: random or argmax(predicted Q_values for current_state)\n",
        "        returns: action\n",
        "    '''\n",
        "    def get_action(self, model, current_state, epsilon=0.3):\n",
        "        \n",
        "        # PLAYING THE NEXT ACTION BY EXPLORATION\n",
        "        if np.random.rand() <= epsilon:\n",
        "            action = np.random.randint(0, number_actions)\n",
        "        \n",
        "        # PLAYING THE NEXT ACTION BY INFERENCE\n",
        "        else:\n",
        "            q_values = model.predict(np.array(np.matrix(current_state)))   #current_state.shape = (4,)\n",
        "            action = np.argmax(q_values[0])\n",
        "        \n",
        "        return action \n",
        "\n",
        "    '''\n",
        "    update_memory:\n",
        "        get the transition made in the current timestep and append it to memory\n",
        "        #transition is defined by [current_state=(state + garbage_info), action, next_state, reward]\n",
        "        state = [state, garbage_quantity[3], garbage_quantity[7], garbage_quantity[21]]\n",
        "        if len of memory is == memory_max_len: pop(first element)\n",
        "    '''\n",
        "\n",
        "    def update_memory(self, transition):\n",
        "        self.memory.append(transition)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    '''\n",
        "    get_batch:\n",
        "        batch_size: 10\n",
        "        input_batch = randomly select min(batch_size, len(memory)) number of states\n",
        "                    from the memory\n",
        "        target_batch = for each of the states in the input_batch, \n",
        "                    corresponding element of the target_batch will be a list containing\n",
        "                    the Q values for all the possible actions for that state\n",
        "                    predicted by the Brain; here we will subsequently have to update\n",
        "                    the Q_value corresponding to the action played (from the memory)\n",
        "                    to reward + discount * max(predicted values for next_state)\n",
        "    '''\n",
        "    def get_batch(self, model, batch_size = 10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_inputs = len(self.memory[0][0])   #fist element of the memory = transition, first element of transition = state, input = state\n",
        "        num_outputs = model.output_shape[-1]\n",
        "        #print(num_inputs, num_outputs)\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\n",
        "        #print(f'inputs: {inputs}, targets: {targets}')\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\n",
        "            current_state2, action2, next_state2, reward2 = self.memory[idx]\n",
        "            #print('inside get batch')\n",
        "            #print(f'current state in get batch = {current_state2}')\n",
        "            inputs[i] = current_state2\n",
        "            #print(f'current state after input line = {current_state2}')\n",
        "            targets[i] = model.predict(np.matrix(current_state2))[0]\n",
        "            #print(f'current state after output line = {current_state2}')\n",
        "            #print(f'output targets of the model: {targets[i]}')\n",
        "            Q_sa = np.max(model.predict(np.matrix(next_state2))[0])\n",
        "            targets[i, action2] = reward2 + self.discount * Q_sa\n",
        "            #print(f'target after Q update: {targets[i]}')\n",
        "            #print('out of getbatch')\n",
        "        return inputs, targets\n",
        "\n",
        "    @staticmethod\n",
        "    def scale(input, scaling_factor = [16, 5, 5, 5]):\n",
        "        scaled_input = input\n",
        "        for i in range(len(scaled_input)):\n",
        "            scaled_input[i] = scaled_input[i]/scaling_factor[i]\n",
        "        return scaled_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSKHFWvvYKhz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "#Training\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZVxmdiB3ni8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed46f59-dff9-4c1a-a2e6-c98be844ceff"
      },
      "source": [
        "#Train the Model and save it\n",
        "'''\n",
        "Create an object of Environment, Brain and DQN Agent\n",
        "create a model of brain class\n",
        "Number_epochs: number of epochs\n",
        "For each epoch:\n",
        "    current_state = environment.reset()\n",
        "    action = DQN.get_action(current_state)\n",
        "    next_state, reward, _, _ = environment.step(action)\n",
        "    update the memory: DQN.update_memory(transition=[current_state, action, next_state, reward])\n",
        "    get the batches of input and output:\n",
        "    batch_input, batch_target = DQN.get_batch()\n",
        "    train on the batch and update the loss:\n",
        "    loss += model.train_on_batch(batch_input, batch_target)\n",
        "    update the total reward\n",
        "    keep a track of the timestep\n",
        "    each epoch can be decided to run for a certain number of timesteps\n",
        "print the results of the training\n",
        "save the model for testing/ simulation    \n",
        "'''\n",
        "\n",
        "#Set the parameters:\n",
        "training_status = 'Test'   #change this to some other value in case training is not desired\n",
        "epsilon = 0.3   #for epsilon delta exploration\n",
        "number_actions = 5\n",
        "number_epochs = 10\n",
        "max_memory = 50\n",
        "discount = 0.9\n",
        "batch_size = 8\n",
        "#model_name = f'/content/gdrive/My Drive/TrashModels/epochs:{epoch:03d}_model_BS8_LP_OPT_1_10_E10_TM4000_MEM50_SRWD.h5'\n",
        "garbage_counter = 0\n",
        "load_trained_model = False\n",
        "\n",
        "'''\n",
        "Set environment parameters and include arguments in the environment class in case\n",
        "parameters of the environment such as:\n",
        "    item{garbage_update\n",
        "    garbage_threshold\n",
        "    living_penalty\n",
        "need to be tuned in the environment object.\n",
        "'''\n",
        "\n",
        "#Create the environment as an object of environmemt class\n",
        "env = TrashEnv()\n",
        "\n",
        "#Create the Deep Neural Network as an object of the Brain class\n",
        "brain = Brain()\n",
        "\n",
        "trained_model =0\n",
        "\n",
        "if load_trained_model:\n",
        "    try:\n",
        "        model = load_model(f'/content/gdrive/My Drive/TrashModels/epochs:{epoch:03d}_model_BS8_LP_OPT_1_10_E10_TM4000_MEM50_SRWD.h5')\n",
        "        print('loaded pre-trained model')\n",
        "        trained_model = 1\n",
        "    except:\n",
        "        print('could not load trained model')\n",
        "        model = brain.model\n",
        "else:\n",
        "    model = brain.model\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "#Create the Agent as an object of the DQN class\n",
        "dqn = DQN(max_memory, discount)\n",
        "\n",
        "#Set mode to training:\n",
        "env.train = training_status\n",
        "\n",
        "#Start the training\n",
        "if env.train == 'Train':\n",
        "    for epoch in range(number_epochs):\n",
        "        total_reward = 0.0\n",
        "        loss = 0.0\n",
        "        timestep=0\n",
        "        current_state_env = env.reset()\n",
        "        print(f'Epoch: {epoch+1}/{number_epochs}')\n",
        "        for _ in tqdm(range(4000)):\n",
        "            #print('\\n')\n",
        "            #print(f'current state = {current_state_env}')\n",
        "            action_env = dqn.get_action(model=model, current_state=current_state_env, epsilon=epsilon)\n",
        "            #print(f'action = {action_env}')\n",
        "            #try: print(f'next state before env.step = {next_state_env}')\n",
        "            #except: pass\n",
        "            next_state_env, reward_env, _, _ = env.step(action_env)\n",
        "            #print(f'next state = {next_state_env}')\n",
        "            transition=[current_state_env, action_env, next_state_env, reward_env]\n",
        "            #print(f'transition: {transition}')\n",
        "            dqn.update_memory(transition)\n",
        "            batch_input_env, batch_target_env = dqn.get_batch(model, batch_size = batch_size)\n",
        "            #print(f'next state after get batch = {next_state_env}')\n",
        "            loss += model.train_on_batch(batch_input_env, batch_target_env)\n",
        "            total_reward +=reward_env\n",
        "            #print(f'next state at end = {next_state_env}')\n",
        "            current_state_env = next_state_env\n",
        "            #print(f'current state at end = {current_state_env}')\n",
        "        #Print the results for the current epoch\n",
        "        print(f'Total loss over the epoch: {loss}')\n",
        "        print(f'Total reward over the epoch: {total_reward}')\n",
        "        print(\"\\n\")\n",
        "        # EARLY STOPPING CAN BE IMPLEMENTED LATER\n",
        "        \n",
        "        # SAVING THE MODEL AFTER EACH EPOCH\n",
        "        if trained_model:\n",
        "            model.save(f'/content/gdrive/My Drive/TrashModels/epochs:{epoch:03d}_model_BS8_LP_OPT_1_10_E10_TM4000_MEM50_SRWD.h5')\n",
        "        else:\n",
        "            model.save(f'/content/gdrive/My Drive/TrashModels/epochs:{epoch:03d}_model_BS8_LP_OPT_1_10_E10_TM4000_MEM50_SRWD.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                320       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 2,565\n",
            "Trainable params: 2,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsH15ZSYYQ-G"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "#Testing\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lga3zaOt3rK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739836fc-7fc9-4a03-ae11-ada524a29111"
      },
      "source": [
        "#Test the model on a similar environment and publish the results\n",
        "'''\n",
        "build a simulation environment\n",
        "load the saved model\n",
        "current_state=environment.reset()\n",
        "run simulation for timestep < some value:\n",
        "    q_values = model.predict(current_state)\n",
        "    action = np.argmax(q_values)\n",
        "    next_state, reward, _, _ = environment.step(action)\n",
        "    current_state = next_state\n",
        "    update reward\n",
        "    #maintain a counter for each time the trash in a particular bin 70%:\n",
        "        if abs(reward[3] at current timestep - reward[3] at last timestep) \n",
        "'''\n",
        "\n",
        "model_name = f'/content/gdrive/My Drive/TrashModels/epochs:006_model_BS8_LP_OPT_1_10_E10_TM4000_MEM50_SRWD.h5'\n",
        "\n",
        "#Create simulation environment\n",
        "sim_env = TrashEnv()\n",
        "\n",
        "#Load the pre-trained model\n",
        "trained_model = load_model(model_name)\n",
        "\n",
        "#Running a 5 day training:\n",
        "\n",
        "total_reward = 0.0\n",
        "loss = 0.0\n",
        "timestep=0\n",
        "current_state = sim_env.reset()\n",
        "while timestep <= 1000:\n",
        "    #print('\\n')\n",
        "    #print(timestep)\n",
        "    q_values = trained_model.predict(np.array(np.matrix(current_state)))\n",
        "    #print(q_values)\n",
        "    action = np.argmax(q_values[0])\n",
        "    next_state, reward, _, _ = sim_env.step(action)\n",
        "    transition=[current_state, action, next_state, reward]\n",
        "    if sim_env.state[0] in sim_env.garbage_locations:\n",
        "        print(f'transition: {transition}')\n",
        "    total_reward +=reward\n",
        "    current_state = next_state\n",
        "    timestep +=1\n",
        "#Print the results for the current epoch\n",
        "print(\"\\n\")\n",
        "print(f'Total reward: {total_reward}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transition: [[16, 19.865245205739328, 20.128740651819395, 19.35742339119697], 2, [15, 19.879487844389576, 20.157691634413364, 0.0], 1.9357423391196973]\n",
            "\n",
            "\n",
            "Total reward: -17.50425766087998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiIxyQvNZGtS"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "#Printing and Visualizing the Results\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu5_cz_ZZNB3"
      },
      "source": [
        "#Print the results\n",
        "#Report the observations\n",
        "#Create a visualization\n",
        "#Write a Conclusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-BF4RSdfOdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "31aebd3e-ac51-41c5-b4ef-b4c9dcfe4688"
      },
      "source": [
        "#Animation of the results\n",
        "\n",
        "#convert results in desired format (TBD)\n",
        "results = []\n",
        "'''\n",
        "#Making the grid\n",
        "N = 6\n",
        "M = 7\n",
        "Roads = np.ones((N, M)) * np.nan    #Empty set\n",
        "fig, ax = plt.subplots(1, 1, tight_layout=True)  #fig + axes\n",
        "Roads_cmap = matplotlib.colors.ListedColormap(['Grey']) #grey for borders\n",
        "for x in range(N + 1):      #draw grid\n",
        "    for y in range(M+1):\n",
        "        ax.axhline(x, lw=2, color='k', zorder=5)\n",
        "        ax.axvline(x, lw=2, color='k', zorder=5)\n",
        "ax.imshow(Roads, interpolation='none', cmap=Roads_cmap, extent=[0, N, 0, M], zorder=0)   #roads\n",
        "ax.axis('off')   #remove axis\n",
        "\n",
        "#Highlighting pre-defined cells (Roads & pickups)\n",
        "\n",
        "#Creating animation\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xdata, ydata = [], []\n",
        "ln, = plt.plot([], [], 'ro')\n",
        "\n",
        "def init():\n",
        "    ax.set_xlim(0, 2*np.pi)\n",
        "    ax.set_ylim(-1, 1)\n",
        "    return ln,\n",
        "\n",
        "def update(frame):\n",
        "    xdata.append(frame)\n",
        "    ydata.append(np.sin(frame))\n",
        "    ln.set_data(xdata, ydata)\n",
        "    return ln,\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 128),\n",
        "                    init_func=init, blit=True)\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#Making the grid\\nN = 6\\nM = 7\\nRoads = np.ones((N, M)) * np.nan    #Empty set\\nfig, ax = plt.subplots(1, 1, tight_layout=True)  #fig + axes\\nRoads_cmap = matplotlib.colors.ListedColormap(['Grey']) #grey for borders\\nfor x in range(N + 1):      #draw grid\\n    for y in range(M+1):\\n        ax.axhline(x, lw=2, color='k', zorder=5)\\n        ax.axvline(x, lw=2, color='k', zorder=5)\\nax.imshow(Roads, interpolation='none', cmap=Roads_cmap, extent=[0, N, 0, M], zorder=0)   #roads\\nax.axis('off')   #remove axis\\n\\n#Highlighting pre-defined cells (Roads & pickups)\\n\\n#Creating animation\\nfrom matplotlib.animation import FuncAnimation\\n\\nfig, ax = plt.subplots()\\nxdata, ydata = [], []\\nln, = plt.plot([], [], 'ro')\\n\\ndef init():\\n    ax.set_xlim(0, 2*np.pi)\\n    ax.set_ylim(-1, 1)\\n    return ln,\\n\\ndef update(frame):\\n    xdata.append(frame)\\n    ydata.append(np.sin(frame))\\n    ln.set_data(xdata, ydata)\\n    return ln,\\n\\nani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 128),\\n                    init_func=init, blit=True)\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbem3AW4GfFc"
      },
      "source": [
        "---\n",
        "#Conclusions\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1p2fUisz8y5"
      },
      "source": [
        "As part of the project a study was made on implementation of Reinforecement Learning for solving a routing problem in order to automate and optimize trash collection in a locality. A grid world environment was created to model the locality, roads and trash locations. A reward strategy was then decided and a Deep Q-Learning process was used to train an agent to detect and automatically select the most optimised path to collect the trash.\n",
        "\n",
        "###Limitations other current work and future scope\n",
        "\n",
        "There were several attempts made by tuning the hyperparameters of the model including modifying reward values, changing reward strategy and improving the neural nework. Different values of memory/ experience replay were also tried out. However, in all cases the agent didn't manage to successfully collect the trash from all locations to maximise the rewards.\n",
        "\n",
        "One of the possible reasons for this can be owed to limited training that was possible with our existing infrastrure. If the model is trained  over million timesteps with a well designed reward function, it would yield much better results.\n",
        "\n",
        "Another improvement can be in the design of the reward strategy. The reward strategies tried out here with a tunable negative living penalty and higher rewards for successfull collection of trash didn't manage to get the agent to all the garbage locations for collection. The agent in its limited train time probably doesn't fully identify a pattern. But the reward strategy may also be more smartly designed to keep ensure that the garbage bins are not allowed to fill beyond 100% and the agent is learning to visit the location when the garbage value is between 70% and 100%. We tried doing this by increasing the living penalty base values but that strategy did not work.\n",
        "\n",
        "Further simulations can be done by maintaining a variable maximum fill rate parameter for different garbage locations.\n",
        "\n",
        "A better neural network architecture which takes the last few steps as input, similar to a recurring neural network idea, may be better for this sort of work. A simple architecture with dense layers like the one used here just doesn't work even when dropout layers and weight initialization techniques were introduced.\n",
        "\n",
        "###Checking performance and benchmarking\n",
        "\n",
        "Our idea was to train a neural network that would predict the next steps based on the state parameters. Once this was trained, the model would be used to run a simulation for say 1000 timesteps on the environment. The total distance covered by the agent in these 1000 timesteps could be easily calculated during the simulation. \n",
        "\n",
        "We planned to then compare this distance covered to a naive strategy where each time a garbage location (say 2) reached a value of 90%, a truck would cover all three garbage locations and collect the trash from there taking the outer loop of the environment from 2 to 15 through 2. After that it would wait at 15 till any location was again (say 2) filled till 90% and it would backtrack from 15 to 2 to wait at 2 now and repeat the whole process. The distance covered during such a naive strategy could also be easily computed.\n",
        "\n",
        "The distances covered by the Reinforcement Learning based agent and by the naive agent would be compared to yield a final value.\n",
        "\n",
        "We could not sadly carry out such a benchmarking as we were unable to finish developing the Deep Q Network to a state where the whole garbage collection was being acceptably performed.\n",
        "\n",
        "###Final Remarks\n",
        "\n",
        "Enhanced efficiency of operational processes can have several advantages in form of cost and energy saving. These would manifest into reduction of externalities such as environmental pollution. Decision making by using reinforcement learning based techniques is a hot topic of discussion now particularly when there is an effort to optimise a process. Our study has given us a great insight into the implementation of Markov Decision Process and Deep Q-learning methods in optimizing a process.\n",
        "\n",
        "###References\n",
        "[1] web.stanford.edu/~mossr/pdf/trashmdp.pdf <br>\n",
        "[2] 2010.02068.pdf (arxiv.org)<br>\n",
        "[3] FULLTEXT01.pdf (diva-portal.org)\n"
      ]
    }
  ]
}